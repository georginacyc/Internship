{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements \n",
    "<ul>\n",
    "    <li>Your function should account for keyboard distance for all characters (even special characters)</li>\n",
    "    <li>Your function should account for levenshtein distance as well. If there are more typos, then the score should lean more towards 1 (since it is unlikely for a user to make so many typos)</li>\n",
    "    <li>Think about if it is more likely to make a horizontal typo vs a vertical typo, you may want to assign a weight to differentiate the typos</li>\n",
    "    <li>Think about the case when the strings have different lengths and how you should handle it</li>\n",
    "    <li>Think about if it is necessary to distinguish if the character is already very far away (e.g wikip9dia.org vs wikip0dia.org), both are most likely typosquats, is there a need for a different score? How many keyboard characters away then should I consider it to be not a typo vs not typo?</li>\n",
    "    <li>Try to think of any other conditions / requirements that I may have missed out, and feel free to suggest any</li>\n",
    "</ul>\n",
    "\n",
    "what about swapped letters, one-too-many letters\n",
    "\n",
    "numbers above qwertyuiop are possible typos, but some may be intended typosquats (i.e. o -> 0; E -> 3 ?)\n",
    "\n",
    "what about when a user presses 2 keys on accident? e.g. wikoipedia -> presses \"o\" and \"k\" when trying to press \"k\"\n",
    "\n",
    "are special chars/homoglyphs legal in the url box?\n",
    "\n",
    "what if they miss a letter?\n",
    "\n",
    "\n",
    "[python-Levenshtein PyPI](https://pypi.org/project/python-Levenshtein/)\n",
    "\n",
    "[euclidean distance using numpy (stack overflow)](https://stackoverflow.com/questions/1401712/how-can-the-euclidean-distance-be-calculated-with-numpy) (may help make calculating ED more efficient?)\n",
    "\n",
    "[top 10 most common TLDs](https://www.statista.com/statistics/265677/number-of-internet-top-level-domains-worldwide/)\n",
    "\n",
    "[domain name regex](https://medium.com/@vaghasiyaharryk/how-to-validate-a-domain-name-using-regular-expression-9ab484a1b430)\n",
    "\n",
    "\n",
    "[Prototype 3 string check](https://stackoverflow.com/questions/774316/python-difflib-highlighting-differences-inline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import *\n",
    "import dnstwist\n",
    "from tldextract import extract\n",
    "import pylev as ls\n",
    "# import Levenshtein as ls\n",
    "import numpy as np\n",
    "import difflib as dl\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.4142135623730951\n"
     ]
    }
   ],
   "source": [
    "keyboard_cartesian = {\n",
    "                        \"1\": {\"y\": -1, \"x\": 0},\n",
    "                        \"2\": {\"y\": -1, \"x\": 1},\n",
    "                        \"3\": {\"y\": -1, \"x\": 2},\n",
    "                        \"4\": {\"y\": -1, \"x\": 3},\n",
    "                        \"5\": {\"y\": -1, \"x\": 4},\n",
    "                        \"6\": {\"y\": -1, \"x\": 5},\n",
    "                        \"7\": {\"y\": -1, \"x\": 6},\n",
    "                        \"8\": {\"y\": -1, \"x\": 7},\n",
    "                        \"9\": {\"y\": -1, \"x\": 8},\n",
    "                        \"0\": {\"y\": -1, \"x\": 9},\n",
    "                        \"-\": {\"y\": -1, \"x\": 10},\n",
    "                        \"q\": {\"y\": 0, \"x\": 0},\n",
    "                        \"w\": {\"y\": 0, \"x\": 1},\n",
    "                        \"e\": {\"y\": 0, \"x\": 2},\n",
    "                        \"r\": {\"y\": 0, \"x\": 3},\n",
    "                        \"t\": {\"y\": 0, \"x\": 4},\n",
    "                        \"y\": {\"y\": 0, \"x\": 5},\n",
    "                        \"u\": {\"y\": 0, \"x\": 6},\n",
    "                        \"i\": {\"y\": 0, \"x\": 7},\n",
    "                        \"o\": {\"y\": 0, \"x\": 8},\n",
    "                        \"p\": {\"y\": 0, \"x\": 9},\n",
    "                        \"a\": {\"y\": 1, \"x\": 0},\n",
    "                        \"s\": {\"y\": 1, \"x\": 1},\n",
    "                        \"d\": {\"y\": 1, \"x\": 2},\n",
    "                        \"f\": {\"y\": 1, \"x\": 3},\n",
    "                        \"g\": {\"y\": 1, \"x\": 4},\n",
    "                        \"h\": {\"y\": 1, \"x\": 5},\n",
    "                        \"j\": {\"y\": 1, \"x\": 6},\n",
    "                        \"k\": {\"y\": 1, \"x\": 7},\n",
    "                        \"l\": {\"y\": 1, \"x\": 8},\n",
    "                        \";\": {\"y\": 2, \"x\": 9},\n",
    "                        \"'\": {\"y\": 2, \"x\": 10},\n",
    "                        \"z\": {\"y\": 2, \"x\": 0},\n",
    "                        \"x\": {\"y\": 2, \"x\": 1},\n",
    "                        \"c\": {\"y\": 2, \"x\": 2},\n",
    "                        \"v\": {\"y\": 2, \"x\": 3},\n",
    "                        \"b\": {\"y\": 2, \"x\": 4},\n",
    "                        \"n\": {\"y\": 2, \"x\": 5},\n",
    "                        \"m\": {\"y\": 2, \"x\": 6},\n",
    "                        \",\": {\"y\": 2, \"x\": 7},\n",
    "                        \".\": {\"y\": 2, \"x\": 8},\n",
    "                        \"/\": {\"y\": 2, \"x\": 9}                   \n",
    "                     }\n",
    "\n",
    "def euclidean_distance(a,b):\n",
    "    X = (keyboard_cartesian[a]['x']-keyboard_cartesian[b]['x'])**2\n",
    "    Y = (keyboard_cartesian[a]['y']-keyboard_cartesian[b]['y'])**2\n",
    "    return sqrt(X+Y)\n",
    "\n",
    "print(euclidean_distance('q', 'w'))\n",
    "print(euclidean_distance('q', 's'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Try\n",
    "simply checking against Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of typosquatted urls generated:  7900\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/44113335/extract-domain-from-url-in-python\n",
    "\n",
    "def replace_special_char(char):\n",
    "    flag = '\"!@#$%^&*()+?_=,<>'':\\\\'\n",
    "    flag_list = [char for char in flag]\n",
    "    if char.isalnum()==False and char in flag:\n",
    "        return 'Z'\n",
    "    return char\n",
    "    \n",
    "# Clean the string first. The extract python library cannot properly extract the domain from URLs with special characters\n",
    "\n",
    "# 1. make string lower case\n",
    "# 2. replace all flagged special characters with 'Z'\n",
    "# 3. extract the domain or TLD\n",
    "# 4. replace all 'Z' with '!'\n",
    "# 5. calculate edit distance\n",
    "\n",
    "def clean_string(url):\n",
    "    # First make the string lowercase\n",
    "    url = url.lower()\n",
    "\n",
    "    # ':' is a flagged character, but if it appears with http or https it is fine\n",
    "    url = url.replace('https://','')\n",
    "    url = url.replace('http://','')\n",
    "    return ''.join([replace_special_char(char) for char in url])\n",
    "    \n",
    "def extract_domain_and_tld(url):\n",
    "    url = clean_string(url)\n",
    "    tsd, td, tsu = extract(url)\n",
    "    final = td + '.' + tsu\n",
    "    return final.replace('Z','!')\n",
    "    \n",
    "def extract_tld(url):\n",
    "    url = clean_string(url)\n",
    "    tsd, td, tsu = extract(url)\n",
    "    return tsu.replace('Z','!')  \n",
    "\n",
    "def extract_sld(url):\n",
    "    url = clean_string(url)\n",
    "    tsd, td, tsu = extract(url)\n",
    "    return td.replace('Z','!')\n",
    "\n",
    "# Theres no need for this container to be mutable and tuples are faster\n",
    "# Index 0 - 9 = most popular to least popular\n",
    "tlds = (\"com\", \"ru\", \"org\", \"net\", \"in\", \"ir\", \"au\", \"uk\", \"de\", \"br\")\n",
    "\n",
    "whitelist = ['https://www.wikipedia.org/']\n",
    "\n",
    "\n",
    "whitelist_domains = []\n",
    "whitelist_slds = []\n",
    "for url in whitelist:\n",
    "    whitelist_domains.append(extract_domain_and_tld(url))\n",
    "    whitelist_slds.append(extract_sld(url))\n",
    "typosquat = []\n",
    "for url in whitelist_domains:\n",
    "    fuzz = dnstwist.DomainFuzz(url)\n",
    "    fuzz.generate()\n",
    "    typosquat.extend([x['domain-name'] for x in fuzz.domains])\n",
    "    \n",
    "#Lookups in sets are much more efficient\n",
    "typosquat = set(typosquat)\n",
    "\n",
    "#Delete the original whitelisted domains from the blacklist set\n",
    "typosquat.difference_update(whitelist_domains)\n",
    "\n",
    "print('Number of typosquatted urls generated: ', len(typosquat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "legit = \"wikipedia.org\"\n",
    "typo = \"wiiipedia.org\" \n",
    "typosqt = \"wikiped1a.org\"\n",
    "\n",
    "def typo_check(url):\n",
    "    for i in range(len(legit)):\n",
    "        if legit[i] != url[i]:\n",
    "            result = euclidean_distance(legit[i], url[i])\n",
    "            if result >= 1.5:\n",
    "                # typosquat\n",
    "                return 1\n",
    "            else:\n",
    "                # typo\n",
    "                return 0\n",
    "\n",
    "print(typo_check(typosqt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T : URL being tested <br>\n",
    "L : Legit URL <br>\n",
    "LD : levenshtein distance <br>\n",
    "ED : euclidean distance <br>\n",
    "\n",
    "Assumptions: <br>\n",
    "<ul >\n",
    "    <li>Given the suspicious URL (S), its genuine URL (G) is known</li>\n",
    "    <li>S has an edit distance of at least 1</li>\n",
    "    <li>There is no reason to press shift; urls are not case sensitive, and the only legal special character is hyphen, which does not require shift</li>\n",
    "    <li>Hyphens are only allowed in between characters in domain names (i.e. domain names cannot start/end with hyphens, and the top level domain cannot contain hyphen)</li>\n",
    "    <li>T is assumed to be a typosquat when it meets one of the following:\n",
    "        <ol>\n",
    "            <li>Contains illegal special characters</li>\n",
    "            <li>Has a Levenshtein distance of more than 1</li>\n",
    "            <li>Is shorter than len(G) - 1</li>\n",
    "            <li>Is longer than len(G) + 1</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "Then, checks:\n",
    "- if there are special characters in T, typosquat (in domain names: hyphens are allowed, underscores are not)\n",
    "- if length of T = length of L + 1, check how far away the extra letter is from the previous letter. was it fat-fingered?\n",
    "- if length of T = length of L, check LD. if > 2, definitely typosquat\n",
    "- if length of T = length of L AND LD <= 2, check ED. if any error has ED > 1.5, typosquat\n",
    "\n",
    "definitely typosquat:\n",
    "- special char present anywhere\n",
    "- len(T) > len(L) + 1\n",
    "- LD > 2\n",
    "- ED > 1.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## is_typo(url) Function\n",
    "Code Flow:\n",
    "- checks if url has any special characters\n",
    "- checks LD of url against legit URL\n",
    "- checks the TLD of url agains legit URL\n",
    "- checks length of url against lenght of legit URL\n",
    "- checks the ED of any wrong char in url against corresponding char in legit URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_typo(url):\n",
    "    l = whitelist_domains[0]\n",
    "    \n",
    "    result = {\"suspicious url\": url, \"original url\": l, \"result\": 0, \"reasons_typosquat\": [], \"reasons_typo\": []}\n",
    "    \n",
    "    url_sld = extract_sld(url)\n",
    "    l_sld = extract_sld(l)\n",
    "    \n",
    "    url_tld = extract_tld(url)\n",
    "    l_tld = extract_tld(l)\n",
    "    \n",
    "    url_len = len(url)\n",
    "    l_len = len(l)\n",
    "    \n",
    "    # checks if illegal special characters are present\n",
    "    if not re.match(\"^[^-][a-zA-Z0-9-]{1,}[^-][.]{1}[a-zA-Z0-9]{1,}$\", url):\n",
    "        result[\"result\"] = 1\n",
    "        result[\"reasons_typosquat\"].append(\"Illegal characters found in url\")\n",
    "    \n",
    "    # checks LD\n",
    "    if ls.levenshtein(url, l) > 1:\n",
    "        result[\"result\"] = 1\n",
    "        result[\"reasons_typosquat\"].append(\"Edit distance more than 1\")\n",
    "    elif ls.levenshtein(url, l) == 1:\n",
    "        result[\"reasons_typo\"].append(\"Edit distance is only one\")\n",
    "    \n",
    "    # checks TLD\n",
    "    if url_tld != l_tld:\n",
    "        url_index = 10\n",
    "        l_index = 10\n",
    "        if url_tld in tlds:\n",
    "            url_index = tlds.index(url_tld)\n",
    "        if l_tld in tlds:\n",
    "            l_index = tlds.index(l_tld)\n",
    "        if l_index > url_index:\n",
    "            result[\"reasons_typo\"].append(\"TLD is more common\")\n",
    "            \n",
    "     \n",
    "    # compares lengths\n",
    "    if url_len > l_len + 1:\n",
    "        result[\"result\"] = 1\n",
    "        result[\"reasons_typosquat\"].append(\"Too long\")\n",
    "    \n",
    "    elif url_len < l_len - 1:\n",
    "        result[\"result\"] = 1\n",
    "        result[\"reasons_typosquat\"].append(\"Too short\")\n",
    "    \n",
    "    elif url_len == l_len + 1:\n",
    "        for i in range(len(l)):\n",
    "            if url[i] != l[i]:\n",
    "                url_left = url[i - 1] if i != 0 else None # char on the left of the wrong/extra char\n",
    "                url_middle = url[i] # wrong/extra char\n",
    "                url_right = url[i + 1] if i + 1 < len(url) else None # char on the right of wrong/extra char          \n",
    "                \n",
    "                if url_left == None:\n",
    "                    if euclidean_distance(url_right, url_middle) > 1.5:\n",
    "                        result[\"result\"] = 1\n",
    "                        result[\"reasons_typosquat\"].append(\"Extra character too far from characters next to it\")\n",
    "                    else:\n",
    "                        result[\"reasons_typo\"].append(\"Extra character is within ED boundary\")\n",
    "                        \n",
    "                elif url_right == None:\n",
    "                    if euclidean_distance(url_left, url_middle) > 1.5:\n",
    "                        result[\"result\"] = 1\n",
    "                        result[\"reasons_typosquat\"].append(\"Extra character too far from characters next to it\")\n",
    "                    else:\n",
    "                        result[\"reasons_typo\"].append(\"Extra character is within ED boundary\")\n",
    "                else:\n",
    "                    if euclidean_distance(url_left, url_middle) > 1.5 and euclidean_distance(url_right, url_middle) > 1.5:\n",
    "                        result[\"result\"] = 1\n",
    "                        result[\"reasons_typosquat\"].append(\"Extra character too far from characters next to it\")\n",
    "                    else:\n",
    "                        result[\"reasons_typo\"].append(\"Extra character is within ED boundary\")\n",
    "                \n",
    "                # prevent the function from running on the rest of the string\n",
    "                break\n",
    "                \n",
    "    elif url_len == l_len:\n",
    "        for i in range(len(l)):\n",
    "            if url[i] != l[i]:\n",
    "                if euclidean_distance(url[i], l[i]) > 1.5:\n",
    "                    result[\"result\"] = 1\n",
    "                    result[\"reasons_typosquat\"].append(\"'{}' key and '{}' key are too far apart\".format(url[i], l[i]))\n",
    "                else:\n",
    "                        result[\"reasons_typo\"].append(\"Wrong character is within ED boundary\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "<u>Potential Results</u>: <br>\n",
    "0: Typo <br>\n",
    "1: Typosquat\n",
    "\n",
    "<u>Test Cases</u>:\n",
    "1. Legit URL (Expected Result: 0)\n",
    "2. Substitute 1 char with SC (Expected Result: 1)\n",
    "3. Substitute 1 char with hyphen (Expected Result: 1)\n",
    "4. Substitute 1 char with wrong char, within ED boundary (Expected Result: 0)\n",
    "5. Substitute 2 char with wrong char, within ED boundary (Expected Result: 0)\n",
    "6. Substitute 3 char with wrong char, within ED boundary(Expected Result: 1)\n",
    "7. Append 1 char, within ED boundary of last char (Expected Result: 0)\n",
    "8. Append 1 char, exceeding ED boundary (Expected Result: 1)\n",
    "9. Append 2 char, within ED boundary(Expected Result: 1)\n",
    "10. Remove 1 char (Expected Result: 1)\n",
    "    \n",
    "<u>Conclusion/Notes</u>:\n",
    "- need to recheck for special characters, as they are not accounted for in the next checks after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>suspicious url</th>\n",
       "      <th>original url</th>\n",
       "      <th>result</th>\n",
       "      <th>reasons_typosquat</th>\n",
       "      <th>reasons_typo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bankofsingapore.com</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>1</td>\n",
       "      <td>[Edit distance more than 1, Too long]</td>\n",
       "      <td>[TLD is more common]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bankofs!ngapore.com</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>1</td>\n",
       "      <td>[Illegal characters found in url, Edit distanc...</td>\n",
       "      <td>[TLD is more common]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bankofsing-pore.com</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>1</td>\n",
       "      <td>[Edit distance more than 1, Too long]</td>\n",
       "      <td>[TLD is more common]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bankofsingaporr.com</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>1</td>\n",
       "      <td>[Edit distance more than 1, Too long]</td>\n",
       "      <td>[TLD is more common]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bankofsingapoer.com</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>1</td>\n",
       "      <td>[Edit distance more than 1, Too long]</td>\n",
       "      <td>[TLD is more common]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bankofsingapier.com</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>1</td>\n",
       "      <td>[Edit distance more than 1, Too long]</td>\n",
       "      <td>[TLD is more common]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bankofsingaporee.com</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>1</td>\n",
       "      <td>[Edit distance more than 1, Too long]</td>\n",
       "      <td>[TLD is more common]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bankofsingaporep.com</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>1</td>\n",
       "      <td>[Edit distance more than 1, Too long]</td>\n",
       "      <td>[TLD is more common]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bankofsingaporeee.com</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>1</td>\n",
       "      <td>[Edit distance more than 1, Too long]</td>\n",
       "      <td>[TLD is more common]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bankofsingapor.com</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>1</td>\n",
       "      <td>[Edit distance more than 1, Too long]</td>\n",
       "      <td>[TLD is more common]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          suspicious url   original url  result  \\\n",
       "0    bankofsingapore.com  wikipedia.org       1   \n",
       "1    bankofs!ngapore.com  wikipedia.org       1   \n",
       "2    bankofsing-pore.com  wikipedia.org       1   \n",
       "3    bankofsingaporr.com  wikipedia.org       1   \n",
       "4    bankofsingapoer.com  wikipedia.org       1   \n",
       "5    bankofsingapier.com  wikipedia.org       1   \n",
       "6   bankofsingaporee.com  wikipedia.org       1   \n",
       "7   bankofsingaporep.com  wikipedia.org       1   \n",
       "8  bankofsingaporeee.com  wikipedia.org       1   \n",
       "9     bankofsingapor.com  wikipedia.org       1   \n",
       "\n",
       "                                   reasons_typosquat          reasons_typo  \n",
       "0              [Edit distance more than 1, Too long]  [TLD is more common]  \n",
       "1  [Illegal characters found in url, Edit distanc...  [TLD is more common]  \n",
       "2              [Edit distance more than 1, Too long]  [TLD is more common]  \n",
       "3              [Edit distance more than 1, Too long]  [TLD is more common]  \n",
       "4              [Edit distance more than 1, Too long]  [TLD is more common]  \n",
       "5              [Edit distance more than 1, Too long]  [TLD is more common]  \n",
       "6              [Edit distance more than 1, Too long]  [TLD is more common]  \n",
       "7              [Edit distance more than 1, Too long]  [TLD is more common]  \n",
       "8              [Edit distance more than 1, Too long]  [TLD is more common]  \n",
       "9              [Edit distance more than 1, Too long]  [TLD is more common]  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cases = [\"bankofsingapore.com\", \n",
    "              \"bankofs!ngapore.com\", \n",
    "              \"bankofsing-pore.com\", \n",
    "              \"bankofsingaporr.com\", \n",
    "              \"bankofsingapoer.com\", \n",
    "              \"bankofsingapier.com\", \n",
    "              \"bankofsingaporee.com\", \n",
    "              \"bankofsingaporep.com\", \n",
    "              \"bankofsingaporeee.com\", \n",
    "              \"bankofsingapor.com\"]\n",
    "\n",
    "test_results = []\n",
    "\n",
    "for url in test_cases:\n",
    "    test_results.append(is_typo(url))\n",
    "    \n",
    "df = pd.DataFrame(test_results)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototype 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_typo(sus_url, legit_url):\n",
    "    result = {\"suspicious url\": sus_url, \"original url\": legit_url, \"result\": 0, \"reasons_typosquat\": [], \"reasons_typo\": []}\n",
    "    \n",
    "    sus_sld = extract_sld(sus_url)\n",
    "    legit_sld = extract_sld(legit_url)\n",
    "    \n",
    "    sus_tld = extract_tld(sus_url)\n",
    "    legit_tld = extract_tld(legit_url)\n",
    "    \n",
    "    sus_len = len(sus_url)\n",
    "    legit_len = len(legit_url)\n",
    "    \n",
    "    # checks length\n",
    "    if legit_len + 1 < sus_len:\n",
    "        result[\"result\"] = 1\n",
    "        result[\"reasons_typosquat\"].append(\"Too long\")\n",
    "        return result\n",
    "    elif sus_len < legit_len - 1:\n",
    "        result[\"result\"] = 1\n",
    "        result[\"reasons_typosquat\"].append(\"Too short\")\n",
    "        return result\n",
    "    \n",
    "    # checks if illegal special characters are present\n",
    "    if not re.match(\"^((?!-)[A-Za-z0–9-]{1,}(?<!-)\\.)+[A-Za-z0-9]{1,}$\", sus_url):\n",
    "        result[\"result\"] = 1\n",
    "        result[\"reasons_typosquat\"].append(\"Illegal characters found in url\")\n",
    "        return result\n",
    "    \n",
    "    # checks TLD; WIP\n",
    "    if sus_tld != legit_tld:\n",
    "        sus_index = 10\n",
    "        legit_index = 10\n",
    "        if sus_tld in tlds:\n",
    "            sus_index = tlds.index(sus_tld)\n",
    "        if legit_tld in tlds:\n",
    "            legit_index = tlds.index(legit_tld)\n",
    "        if legit_index > sus_index:\n",
    "            result[\"reasons_typo\"].append(\"TLD is more common\")\n",
    "            \n",
    "            # reassign values so next checks will only be on sld, as tld mistakes have already been ruled out\n",
    "            sus_url = sus_sld\n",
    "            legit_url = legit_sld\n",
    "            sus_len = len(sus_url)\n",
    "            legit_len = len(legit_url)\n",
    "    \n",
    "    \n",
    "    # levenshtein distance check\n",
    "    if ls.levenshtein(sus_url, legit_url) > 1:\n",
    "        result[\"result\"] = 1\n",
    "        result[\"reasons_typosquat\"].append(\"Edit distance more than 1\")\n",
    "        return result\n",
    "    else:\n",
    "        # length checks\n",
    "        if sus_len == legit_len:\n",
    "            for i in range(len(legit_url)):\n",
    "                if sus_url[i] != legit_url[i]:\n",
    "                    if euclidean_distance(sus_url[i], legit_url[i]) > 1.5:\n",
    "                        result[\"result\"] = 1\n",
    "                        result[\"reasons_typosquat\"].append(\"'{}' key and '{}' key are too far apart\".format(sus_url[i], legit_url[i]))\n",
    "                    else:\n",
    "                            result[\"reasons_typo\"].append(\"Wrong character is within ED boundary\")\n",
    "        \n",
    "        elif sus_len == legit_len + 1:\n",
    "            for i in range(len(legit_url)):\n",
    "                if sus_url[i] != legit_url[i]:\n",
    "                    sus_left = sus_url[i - 1] if i != 0 else None # char on the left of the wrong/extra char\n",
    "                    sus_middle = sus_url[i] # wrong/extra char\n",
    "                    sus_right = sus_url[i + 1] if i + 1 < len(url) else None # char on the right of wrong/extra char          \n",
    "\n",
    "                    if sus_left == None:\n",
    "                        if euclidean_distance(sus_right, sus_middle) > 1.5:\n",
    "                            result[\"result\"] = 1\n",
    "                            result[\"reasons_typosquat\"].append(\"Extra character too far from characters next to it\")\n",
    "                        else:\n",
    "                            result[\"reasons_typo\"].append(\"Extra character is within ED boundary\")\n",
    "\n",
    "                    elif sus_right == None:\n",
    "                        if euclidean_distance(sus_left, sus_middle) > 1.5:\n",
    "                            result[\"result\"] = 1\n",
    "                            result[\"reasons_typosquat\"].append(\"Extra character too far from characters next to it\")\n",
    "                        else:\n",
    "                            result[\"reasons_typo\"].append(\"Extra character is within ED boundary\")\n",
    "                    else:\n",
    "                        if euclidean_distance(sus_left, sus_middle) > 1.5 and euclidean_distance(sus_right, sus_middle) > 1.5:\n",
    "                            result[\"result\"] = 1\n",
    "                            result[\"reasons_typosquat\"].append(\"Extra character too far from characters next to it\")\n",
    "                        else:\n",
    "                            result[\"reasons_typo\"].append(\"Extra character is within ED boundary\")\n",
    "\n",
    "                    # prevent the function from running on the rest of the string\n",
    "                    break\n",
    "        \n",
    "        elif sus_len == legit_len - 1:\n",
    "            result[\"reasons_typo\"].append(\"Only missing 1 character\")\n",
    "            \n",
    "        if len(result[\"reasons_typo\"]) > 1:\n",
    "            result[\"result\"] = 1\n",
    "            result[\"reasons_typosquat\"].append(\"Too many typos\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>suspicious url</th>\n",
       "      <th>original url</th>\n",
       "      <th>result</th>\n",
       "      <th>reasons_typosquat</th>\n",
       "      <th>reasons_typo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wikipediaaaaa.org</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>1</td>\n",
       "      <td>[Too long]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wikipe.org</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>1</td>\n",
       "      <td>[Too short]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wikipedi@.org</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>1</td>\n",
       "      <td>[Illegal characters found in url]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wikipedia.com</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[TLD is more common]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wikipedis.com</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>1</td>\n",
       "      <td>[Too many typos]</td>\n",
       "      <td>[TLD is more common, Wrong character is within...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>wikipedai.org</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>1</td>\n",
       "      <td>[Edit distance more than 1]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wikipedih.org</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>1</td>\n",
       "      <td>['h' key and 'a' key are too far apart]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>wikipediap.org</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>1</td>\n",
       "      <td>[Extra character too far from characters next ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      suspicious url   original url  result  \\\n",
       "0      wikipedia.org  wikipedia.org       0   \n",
       "1  wikipediaaaaa.org  wikipedia.org       1   \n",
       "2         wikipe.org  wikipedia.org       1   \n",
       "3      wikipedi@.org  wikipedia.org       1   \n",
       "4      wikipedia.com  wikipedia.org       0   \n",
       "5      wikipedis.com  wikipedia.org       1   \n",
       "6      wikipedai.org  wikipedia.org       1   \n",
       "7      wikipedih.org  wikipedia.org       1   \n",
       "8     wikipediap.org  wikipedia.org       1   \n",
       "\n",
       "                                   reasons_typosquat  \\\n",
       "0                                                 []   \n",
       "1                                         [Too long]   \n",
       "2                                        [Too short]   \n",
       "3                  [Illegal characters found in url]   \n",
       "4                                                 []   \n",
       "5                                   [Too many typos]   \n",
       "6                        [Edit distance more than 1]   \n",
       "7            ['h' key and 'a' key are too far apart]   \n",
       "8  [Extra character too far from characters next ...   \n",
       "\n",
       "                                        reasons_typo  \n",
       "0                                                 []  \n",
       "1                                                 []  \n",
       "2                                                 []  \n",
       "3                                                 []  \n",
       "4                               [TLD is more common]  \n",
       "5  [TLD is more common, Wrong character is within...  \n",
       "6                                                 []  \n",
       "7                                                 []  \n",
       "8                                                 []  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "legit_url = whitelist_domains[0]\n",
    "test_urls = [\"wikipedia.org\", \"wikipediaaaaa.org\", \"wikipe.org\", \"wikipedi@.org\", \"wikipedia.com\", \"wikipedis.com\", \"wikipedai.org\", \"wikipedih.org\", \"wikipediap.org\"]\n",
    "test_results = []\n",
    "\n",
    "for url in test_urls:\n",
    "    test_results.append(is_typo(url, legit_url))\n",
    "\n",
    "df = pd.DataFrame(test_results)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototype 3\n",
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Input - 1 URL\n",
    "# Output - True / False\n",
    "\n",
    "# e.g contains_special_characters(wikipedia.org) \n",
    "# expected output False\n",
    "\n",
    "# e.g contains_special_characters(wikipedi@.org) \n",
    "# expected output True\n",
    "\n",
    "def contains_special_characters(url):\n",
    "    return not re.match(\"^((?!-)[A-Za-z0-9-]{1,}(?<!-)\\.)+[A-Za-z0-9]{1,}$\", url)\n",
    "\n",
    "\n",
    "# Helper function to find out if the first TLD is more common than the second TLD\n",
    "\n",
    "# Input - 2 TLDs\n",
    "# Output - True / False\n",
    "\n",
    "# e.g exact_tld_swap(com, org) \n",
    "# expected output True\n",
    "\n",
    "# e.g exact_tld_swap(org, com) \n",
    "# expected output False\n",
    "\n",
    "def tld_more_common(tld1, tld2):    \n",
    "    tld1_index = 10\n",
    "    tld2_index = 10\n",
    "    if tld1 in tlds:\n",
    "        tld1_index = tlds.index(tld1)\n",
    "    if tld2 in tlds:\n",
    "        tld2_index = tlds.index(tld2)\n",
    "    if tld2_index > tld1_index:\n",
    "        return True\n",
    "    elif tld2_index < tld1_index:\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "# Helper function to perform various euclidean distance checks based off of the lengths of both URLs\n",
    "\n",
    "# Input - 2 URLs\n",
    "# Output - dictionary of results\n",
    "\n",
    "# e.g. edit_check(\"w1k1pedia.org\", \"wikipedia.org\")\n",
    "# expected output: {'result': 1, 'reasons_typosquat': [\"'1' key and 'i' key are too far apart\", \"'1' key and 'i' key are too far apart\"], 'reasons_typo': []}\n",
    "\n",
    "def edit_check(sus_url, legit_url):\n",
    "    result = {\"result\": 0, \"reasons_typosquat\": [], \"reasons_typo\": []}\n",
    "    \n",
    "    # retrieving length of both URLs\n",
    "    sus_len = len(sus_url)\n",
    "    legit_len = len(legit_url)\n",
    "    \n",
    "    # if lengths are equal, two characters were swapped\n",
    "    if sus_len == legit_len:\n",
    "\n",
    "        # running through each letter in both sides\n",
    "        for i in range(len(legit_url)): \n",
    "\n",
    "            # if letters dont match, check ED\n",
    "            if sus_url[i] != legit_url[i]: \n",
    "                if euclidean_distance(sus_url[i], legit_url[i]) > 1.5:\n",
    "                    result[\"result\"] = 1\n",
    "                    result[\"reasons_typosquat\"].append(\"'{}' key and '{}' key are too far apart\".format(sus_url[i], legit_url[i]))\n",
    "                else:\n",
    "                    result[\"reasons_typo\"].append(\"Wrong character is within ED boundary\")\n",
    "\n",
    "    # if sus_len == legit_len - 1, missing 1 char, swapped 1\n",
    "    # if sus_len == legit_len + 1, extra 1 char, swapped 1\n",
    "    elif sus_len == legit_len - 1 or sus_len == legit_len + 1:\n",
    "\n",
    "        # creating an object that compares both urls\n",
    "        seqm = dl.SequenceMatcher(None, sus_url, legit_url)\n",
    "\n",
    "        # get_opcodes() gets the \"differences\" between each url, or the steps required for first url to match second url\n",
    "        for opcode, a0, a1, b0, b1 in seqm.get_opcodes():\n",
    "            # a : first url\n",
    "            # b : second url\n",
    "            # a0, a1 | b0, b1 : index range holding the characters being compared\n",
    "            # opcode : \"equal\", \"insert\", \"delete\", \"replace\" -- indicates the action require to turn a to b\n",
    "\n",
    "            # if a character has to be deleted, means it's an extra character\n",
    "            if opcode == 'delete':\n",
    "\n",
    "                # retrieving the extra char, and the chars on its left and right\n",
    "                extra_char = seqm.a[a0: a1]\n",
    "                left_char = seqm.a[a0-1: a1-1]\n",
    "                right_char = seqm.a[a0+1: a1+1]\n",
    "\n",
    "                # if the left char is the end of the url, check only the right char\n",
    "                if left_char == \"\":\n",
    "                    if euclidean_distance(right_char, extra_char) > 1.5:\n",
    "                        result[\"result\"] = 1\n",
    "                        result[\"reasons_typosquat\"].append(\"Extra character too far from characters next to it\")\n",
    "                    else:\n",
    "                        result[\"reasons_typo\"].append(\"Extra character is within ED boundary\")\n",
    "\n",
    "                # if the right char is the end of the url, check only the left char\n",
    "                elif right_char == \"\":\n",
    "                    if euclidean_distance(left_char, extra_char) > 1.5:\n",
    "                        result[\"result\"] = 1\n",
    "                        result[\"reasons_typosquat\"].append(\"Extra character too far from characters next to it\")\n",
    "                    else:\n",
    "                        result[\"reasons_typo\"].append(\"Extra character is within ED boundary\")\n",
    "\n",
    "                # check both left and right\n",
    "                else:\n",
    "                    if euclidean_distance(left_char, extra_char) > 1.5 and euclidean_distance(right_char, extra_char) > 1.5:\n",
    "                        result[\"result\"] = 1\n",
    "                        result[\"reasons_typosquat\"].append(\"Extra character too far from characters next to it\")\n",
    "                    else:\n",
    "                        result[\"reasons_typo\"].append(\"Extra character is within ED boundary\")\n",
    "\n",
    "            # if a character has to be replaced, means its a swapped character\n",
    "            elif opcode == 'replace':\n",
    "                wrong_char = seqm.a[a0: a1]\n",
    "                correct_char = seqm.b[b0:b1]\n",
    "\n",
    "                # if the wrong char + extra/missing char are side by side, correct_char will hold both correct characters in one string\n",
    "                if len(correct_char) == 2:\n",
    "\n",
    "                    # retrieving each of the correct chars, and the chars on their left and right\n",
    "                    left_char = seqm.b[b0-1:b1-2]\n",
    "                    correct1 = correct_char[0]\n",
    "                    correct2 = correct_char[1]\n",
    "                    right_char = seqm.b[b0+2:b1+1]\n",
    "\n",
    "                    if euclidean_distance(left_char, wrong_char) > 1.5 and euclidean_distance(correct1, wrong_char) > 1.5 and euclidean_distance(correct2, wrong_char) > 1.5 and euclidean_distance(right_char, wrong_char) > 1.5:\n",
    "                        result[\"result\"] = 1\n",
    "                        result[\"reasons_typosquat\"].append(\"Extra character too far from characters next to it\")                            \n",
    "                    else:\n",
    "                        result[\"reasons_typo\"].append(\"Extra character is within ED boundary\")\n",
    "\n",
    "                else:\n",
    "                    if euclidean_distance(wrong_char, correct_char) > 1.5:\n",
    "                        result[\"result\"] = 1\n",
    "                        result[\"reasons_typosquat\"].append(\"'{}' key and '{}' key are too far apart\".format(wrong_char, correct_char))\n",
    "                    else:\n",
    "                        result[\"reasons_typo\"].append(\"Wrong character is within ED boundary\")\n",
    "\n",
    "    elif sus_len == legit_len + 2:\n",
    "        # creating an object that compares both urls\n",
    "        seqm = dl.SequenceMatcher(None, sus_url, legit_url)\n",
    "\n",
    "        # get_opcodes() gets the \"differences\" between each url, or the steps required for first url to match second url\n",
    "        for opcode, a0, a1, b0, b1 in seqm.get_opcodes():\n",
    "            # a : first url\n",
    "            # b : second url\n",
    "            # a0, a1 | b0, b1 : index range holding the characters being compared\n",
    "            # opcode : \"equal\", \"insert\", \"delete\", \"replace\" -- indicates the action require to turn a to b\n",
    "\n",
    "            # if a character has to be deleted, means it's an extra character\n",
    "            if opcode == 'delete':\n",
    "                extra_char = seqm.a[a0: a1]\n",
    "\n",
    "                # if the extra chars are next to each other, extra_char will hold both extra chars\n",
    "                if len(extra_char) == 2:\n",
    "\n",
    "                    # retrieving the extra characters individually, and the chars on their left and right\n",
    "                    left_char = seqm.a[a0-1:a1-2]\n",
    "                    extra1 = extra_char[0]\n",
    "                    extra2 = extra_char[1]\n",
    "                    right_char = seqm.a[a0+2:a1+1]\n",
    "\n",
    "                    # if the char on the left is empty, check only against the right\n",
    "                    if left_char == \"\":\n",
    "                        if euclidean_distance(right_char, extra1) > 1.5 and euclidean_distance(extra1, extra2) > 1.5:\n",
    "                            result[\"result\"] = 1\n",
    "                            result[\"reasons_typosquat\"].append(\"Extra character too far from characters next to it\")\n",
    "                        else:\n",
    "                            result[\"reasons_typo\"].append(\"Extra character is within ED boundary\")\n",
    "\n",
    "                    # if the char on the right is empty, check only against the left\n",
    "                    elif right_char == \"\":\n",
    "                        if euclidean_distance(left_char, extra1) > 1.5 and euclidean_distance(extra1, extra2) > 1.5:\n",
    "                            result[\"result\"] = 1\n",
    "                            result[\"reasons_typosquat\"].append(\"Extra character too far from characters next to it\")\n",
    "                        else:\n",
    "                            result[\"reasons_typo\"].append(\"Extra character is within ED boundary\")\n",
    "\n",
    "                    # check both left and right\n",
    "                    else:                        \n",
    "                        # checking the ED of extra chars against each char next to it on the url\n",
    "                        if euclidean_distance(left_char, extra1) > 1.5 and euclidean_distance(extra1, extra2) > 1.5 and euclidean_distance(extra2, right_char) > 1.5:\n",
    "                            result[\"result\"] = 1\n",
    "                            result[\"reasons_typosquat\"].append(\"Extra character too far from characters next to it\")                          \n",
    "                        else:\n",
    "                            result[\"reasons_typo\"].append(\"Extra character is within ED boundary\")\n",
    "                else:\n",
    "\n",
    "                    # retrieving the chars on the left and right of the extra char\n",
    "                    left_char = seqm.a[a0-1: a1-1]\n",
    "                    right_char = seqm.a[a0+1: a1+1]\n",
    "\n",
    "                    # if the char on the left is empty, check only against the right\n",
    "                    if left_char == \"\":\n",
    "                        if euclidean_distance(right_char, extra_char) > 1.5:\n",
    "                            result[\"result\"] = 1\n",
    "                            result[\"reasons_typosquat\"].append(\"Extra character too far from characters next to it\")\n",
    "                        else:\n",
    "                            result[\"reasons_typo\"].append(\"Extra character is within ED boundary\")\n",
    "\n",
    "                    # if the char on the right is empty, check only against the left\n",
    "                    elif right_char == \"\":\n",
    "                        if euclidean_distance(left_char, extra_char) > 1.5:\n",
    "                            result[\"result\"] = 1\n",
    "                            result[\"reasons_typosquat\"].append(\"Extra character too far from characters next to it\")\n",
    "                        else:\n",
    "                            result[\"reasons_typo\"].append(\"Extra character is within ED boundary\")\n",
    "\n",
    "                    # check both left and right\n",
    "                    else:\n",
    "                        if euclidean_distance(left_char, extra_char) > 1.5 and euclidean_distance(right_char, extra_char) > 1.5:\n",
    "                            result[\"result\"] = 1\n",
    "                            result[\"reasons_typosquat\"].append(\"Extra character too far from characters next to it\")\n",
    "                        else:\n",
    "                            result[\"reasons_typo\"].append(\"Extra character is within ED boundary\")\n",
    "\n",
    "\n",
    "    # if sus_len == legit_len -2, missing 2 characters, TYPO\n",
    "    elif sus_len == legit_len - 2:\n",
    "        result[\"reasons_typo\"].append(\"Only 2 characters are missing\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main program codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_typo(sus_url, legit_url):\n",
    "    # preparing \n",
    "    result = {\"suspicious url\": sus_url, \"original url\": legit_url, \"result\": 0, \"reasons_typosquat\": [], \"reasons_typo\": []}\n",
    "    \n",
    "    # extracting SLD from both URLs\n",
    "    sus_sld = extract_sld(sus_url)\n",
    "    legit_sld = extract_sld(legit_url)\n",
    "    \n",
    "    # extracting TLD from both URLs\n",
    "    sus_tld = extract_tld(sus_url)\n",
    "    legit_tld = extract_tld(legit_url)\n",
    "    \n",
    "    # retrieving length of both URLs\n",
    "    sus_len = len(sus_url)\n",
    "    legit_len = len(legit_url)\n",
    "    \n",
    "    # check for illegal special characters\n",
    "    if contains_special_characters(sus_url):\n",
    "        result[\"result\"] = 1\n",
    "        result[\"reasons_typosquat\"].append(\"Illegal characters found in url\")\n",
    "        return result\n",
    "    \n",
    "    # check for exact TLD swap\n",
    "    if sus_sld == legit_sld and sus_tld != legit_tld:\n",
    "        if tld_more_common(sus_tld, legit_tld):\n",
    "            result[\"reasons_typo\"].append(\"TLD is more common\")\n",
    "            return result\n",
    "        else:\n",
    "            result[\"result\"] = 1\n",
    "            result[\"reasons_typosquat\"].append(\"TLD is less common\")\n",
    "            return result\n",
    "    \n",
    "    # check edit distance\n",
    "    ld = ls.levenshtein(sus_url, legit_url)\n",
    "    \n",
    "    # if only 1 edit\n",
    "    if ld == 1:\n",
    "        result[\"result\"] = 1\n",
    "        result[\"reasons_typosquat\"].append(\"Edit distance is only 1\")\n",
    "        return result\n",
    "    \n",
    "    # if exactly 2 edits\n",
    "    elif ld == 2:\n",
    "        res = edit_check(sus_url, legit_url)\n",
    "        \n",
    "        result[\"reasons_typosquat\"].extend(res[\"reasons_typosquat\"])\n",
    "        result[\"reasons_typo\"].extend(res[\"reasons_typo\"])\n",
    "        \n",
    "        if res[\"result\"] == 1:\n",
    "            result[\"result\"] = 1\n",
    "            return result\n",
    "    \n",
    "    # if 3 or more edits\n",
    "    else:\n",
    "        result[\"result\"] = \"Inconclusive\"\n",
    "        return result\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>suspicious url</th>\n",
       "      <th>original url</th>\n",
       "      <th>result</th>\n",
       "      <th>reasons_typosquat</th>\n",
       "      <th>reasons_typo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wikipedi@.org</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>1</td>\n",
       "      <td>[Illegal characters found in url]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wikipedia.com</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[TLD is more common]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wkpedia.org</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Only 2 characters are missing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>w1k1ped1a.org</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>Inconclusive</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>w1k1pedia.org</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>1</td>\n",
       "      <td>['1' key and 'i' key are too far apart, '1' key and 'i' key are too far apart]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wkipedis.org</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Wrong character is within ED boundary]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>wiikipedis.org</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Extra character is within ED boundary, Wrong character is within ED boundary]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wikipedg.org</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>1</td>\n",
       "      <td>[Extra character too far from characters next to it]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>wikipedv.org</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Extra character is within ED boundary]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>wikipedia.orggg</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Extra character is within ED boundary]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>aikipedia.orf</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Wrong character is within ED boundary, Wrong character is within ED boundary]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     suspicious url   original url        result  \\\n",
       "0     wikipedi@.org  wikipedia.org             1   \n",
       "1     wikipedia.com  wikipedia.org             0   \n",
       "2       wkpedia.org  wikipedia.org             0   \n",
       "3     w1k1ped1a.org  wikipedia.org  Inconclusive   \n",
       "4     w1k1pedia.org  wikipedia.org             1   \n",
       "5      wkipedis.org  wikipedia.org             0   \n",
       "6    wiikipedis.org  wikipedia.org             0   \n",
       "7      wikipedg.org  wikipedia.org             1   \n",
       "8      wikipedv.org  wikipedia.org             0   \n",
       "9   wikipedia.orggg  wikipedia.org             0   \n",
       "10    aikipedia.orf  wikipedia.org             0   \n",
       "\n",
       "                                                                 reasons_typosquat  \\\n",
       "0                                                [Illegal characters found in url]   \n",
       "1                                                                               []   \n",
       "2                                                                               []   \n",
       "3                                                                               []   \n",
       "4   ['1' key and 'i' key are too far apart, '1' key and 'i' key are too far apart]   \n",
       "5                                                                               []   \n",
       "6                                                                               []   \n",
       "7                             [Extra character too far from characters next to it]   \n",
       "8                                                                               []   \n",
       "9                                                                               []   \n",
       "10                                                                              []   \n",
       "\n",
       "                                                                      reasons_typo  \n",
       "0                                                                               []  \n",
       "1                                                             [TLD is more common]  \n",
       "2                                                  [Only 2 characters are missing]  \n",
       "3                                                                               []  \n",
       "4                                                                               []  \n",
       "5                                          [Wrong character is within ED boundary]  \n",
       "6   [Extra character is within ED boundary, Wrong character is within ED boundary]  \n",
       "7                                                                               []  \n",
       "8                                          [Extra character is within ED boundary]  \n",
       "9                                          [Extra character is within ED boundary]  \n",
       "10  [Wrong character is within ED boundary, Wrong character is within ED boundary]  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "legit_url = whitelist_domains[0]\n",
    "test_urls = [\"wikipedi@.org\", \"wikipedia.com\", \"wkpedia.org\", \"w1k1ped1a.org\", \"w1k1pedia.org\", \"wkipedis.org\", \"wiikipedis.org\", \"wikipedg.org\", \"wikipedv.org\", \"wikipedia.orggg\", \"aikipedia.orf\"]\n",
    "test_results = []\n",
    "\n",
    "for url in test_urls:\n",
    "    test_results.append(is_typo(url, legit_url))\n",
    "    \n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df = pd.DataFrame(test_results)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
