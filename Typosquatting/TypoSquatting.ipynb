{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements \n",
    "<ul>\n",
    "    <li>Your function should account for keyboard distance for all characters (even special characters)</li>\n",
    "    <li>Your function should account for levenshtein distance as well. If there are more typos, then the score should lean more towards 1 (since it is unlikely for a user to make so many typos)</li>\n",
    "    <li>Think about if it is more likely to make a horizontal typo vs a vertical typo, you may want to assign a weight to differentiate the typos</li>\n",
    "    <li>Think about the case when the strings have different lengths and how you should handle it</li>\n",
    "    <li>Think about if it is necessary to distinguish if the character is already very far away (e.g wikip9dia.org vs wikip0dia.org), both are most likely typosquats, is there a need for a different score? How many keyboard characters away then should I consider it to be not a typo vs not typo?</li>\n",
    "    <li>Try to think of any other conditions / requirements that I may have missed out, and feel free to suggest any</li>\n",
    "</ul>\n",
    "\n",
    "what about swapped letters, one-too-many letters\n",
    "\n",
    "numbers above qwertyuiop are possible typos, but some may be intended typosquats (i.e. o -> 0; E -> 3 ?)\n",
    "\n",
    "what about when a user presses 2 keys on accident? e.g. wikoipedia -> presses \"o\" and \"k\" when trying to press \"k\"\n",
    "\n",
    "are special chars/homoglyphs legal in the url box?\n",
    "\n",
    "what if they miss a letter?\n",
    "\n",
    "\n",
    "[python-Levenshtein PyPI](https://pypi.org/project/python-Levenshtein/)\n",
    "\n",
    "[euclidean distance using numpy (stack overflow)](https://stackoverflow.com/questions/1401712/how-can-the-euclidean-distance-be-calculated-with-numpy) (may help make calculating ED more efficient?)\n",
    "\n",
    "[top 10 most common TLDs](https://www.statista.com/statistics/265677/number-of-internet-top-level-domains-worldwide/)\n",
    "\n",
    "[domain name regex](https://medium.com/@vaghasiyaharryk/how-to-validate-a-domain-name-using-regular-expression-9ab484a1b430)\n",
    "\n",
    "\n",
    "[Prototype 3 string check](https://stackoverflow.com/questions/774316/python-difflib-highlighting-differences-inline)\n",
    "\n",
    "[regex to extract subdomain and domain](https://stackoverflow.com/questions/56157896/regex-for-extracting-domains-and-subdomains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import *\n",
    "import dnstwist\n",
    "import pylev as ls\n",
    "# import Levenshtein as ls\n",
    "import numpy as np\n",
    "import difflib as dl\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "ked_boundary = 1.5\n",
    "allow_transposition = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyboard Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.4142135623730951\n"
     ]
    }
   ],
   "source": [
    "keyboard_cartesian = {\n",
    "                        \"1\": {\"y\": -1, \"x\": 0},\n",
    "                        \"2\": {\"y\": -1, \"x\": 1},\n",
    "                        \"3\": {\"y\": -1, \"x\": 2},\n",
    "                        \"4\": {\"y\": -1, \"x\": 3},\n",
    "                        \"5\": {\"y\": -1, \"x\": 4},\n",
    "                        \"6\": {\"y\": -1, \"x\": 5},\n",
    "                        \"7\": {\"y\": -1, \"x\": 6},\n",
    "                        \"8\": {\"y\": -1, \"x\": 7},\n",
    "                        \"9\": {\"y\": -1, \"x\": 8},\n",
    "                        \"0\": {\"y\": -1, \"x\": 9},\n",
    "                        \"-\": {\"y\": -1, \"x\": 10},\n",
    "                        \"q\": {\"y\": 0, \"x\": 0},\n",
    "                        \"w\": {\"y\": 0, \"x\": 1},\n",
    "                        \"e\": {\"y\": 0, \"x\": 2},\n",
    "                        \"r\": {\"y\": 0, \"x\": 3},\n",
    "                        \"t\": {\"y\": 0, \"x\": 4},\n",
    "                        \"y\": {\"y\": 0, \"x\": 5},\n",
    "                        \"u\": {\"y\": 0, \"x\": 6},\n",
    "                        \"i\": {\"y\": 0, \"x\": 7},\n",
    "                        \"o\": {\"y\": 0, \"x\": 8},\n",
    "                        \"p\": {\"y\": 0, \"x\": 9},\n",
    "                        \"a\": {\"y\": 1, \"x\": 0},\n",
    "                        \"s\": {\"y\": 1, \"x\": 1},\n",
    "                        \"d\": {\"y\": 1, \"x\": 2},\n",
    "                        \"f\": {\"y\": 1, \"x\": 3},\n",
    "                        \"g\": {\"y\": 1, \"x\": 4},\n",
    "                        \"h\": {\"y\": 1, \"x\": 5},\n",
    "                        \"j\": {\"y\": 1, \"x\": 6},\n",
    "                        \"k\": {\"y\": 1, \"x\": 7},\n",
    "                        \"l\": {\"y\": 1, \"x\": 8},\n",
    "                        \";\": {\"y\": 2, \"x\": 9},\n",
    "                        \"'\": {\"y\": 2, \"x\": 10},\n",
    "                        \"z\": {\"y\": 2, \"x\": 0},\n",
    "                        \"x\": {\"y\": 2, \"x\": 1},\n",
    "                        \"c\": {\"y\": 2, \"x\": 2},\n",
    "                        \"v\": {\"y\": 2, \"x\": 3},\n",
    "                        \"b\": {\"y\": 2, \"x\": 4},\n",
    "                        \"n\": {\"y\": 2, \"x\": 5},\n",
    "                        \"m\": {\"y\": 2, \"x\": 6},\n",
    "                        \",\": {\"y\": 2, \"x\": 7},\n",
    "                        \".\": {\"y\": 2, \"x\": 8},\n",
    "                        \"/\": {\"y\": 2, \"x\": 9}                   \n",
    "                     }\n",
    "\n",
    "def euclidean_distance(a,b):\n",
    "    X = (keyboard_cartesian[a]['x']-keyboard_cartesian[b]['x'])**2\n",
    "    Y = (keyboard_cartesian[a]['y']-keyboard_cartesian[b]['y'])**2\n",
    "    return sqrt(X+Y)\n",
    "\n",
    "print(euclidean_distance('q', 'w'))\n",
    "print(euclidean_distance('q', 's'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of typosquatted urls generated:  7900\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/44113335/extract-domain-from-url-in-python\n",
    "\n",
    "def replace_special_char(char):\n",
    "    flag = '\"!@#$%^&*()+?_=,<>'':\\\\'\n",
    "    flag_list = [char for char in flag]\n",
    "    if char.isalnum()==False and char in flag:\n",
    "        return 'Z'\n",
    "    return char\n",
    "    \n",
    "# Clean the string first. The extract python library cannot properly extract the domain from URLs with special characters\n",
    "\n",
    "# 1. make string lower case\n",
    "# 2. replace all flagged special characters with 'Z'\n",
    "# 3. extract the domain or TLD\n",
    "# 4. replace all 'Z' with '!'\n",
    "# 5. calculate edit distance\n",
    "\n",
    "def clean_string(url):\n",
    "    # First make the string lowercase\n",
    "    url = url.lower()\n",
    "\n",
    "    # ':' is a flagged character, but if it appears with http or https it is fine\n",
    "    url = url.replace('https://','')\n",
    "    url = url.replace('http://','')\n",
    "    return ''.join([replace_special_char(char) for char in url])\n",
    "    \n",
    "def extract_subdomain_and_domain(url):\n",
    "    url = clean_string(url)\n",
    "    rgx = r\"^(?:https?:\\/\\/)?(?:[^@\\n]+@)?(?:www\\.)?([^:\\/\\n?]+)\"\n",
    "    result = re.search(rgx, url).group(1)\n",
    "    return result.replace('Z','!')\n",
    "\n",
    "def extract_sld(url):\n",
    "    url = clean_string(url)\n",
    "    url_split = url.split(\".\")\n",
    "    return url_split[-2].replace('Z','!')\n",
    "    \n",
    "def extract_tld(url):\n",
    "    url = clean_string(url)\n",
    "    url_split = url.split(\".\")\n",
    "    return url_split[-1].replace('Z','!')\n",
    "\n",
    "# Theres no need for this container to be mutable and tuples are faster\n",
    "# Index 0 - 9 = most popular to least popular\n",
    "tlds = (\"com\", \"ru\", \"org\", \"net\", \"in\", \"ir\", \"au\", \"uk\", \"de\", \"br\")\n",
    "\n",
    "whitelist = ['https://www.wikipedia.org/']\n",
    "\n",
    "\n",
    "whitelist_domains = []\n",
    "whitelist_slds = []\n",
    "for url in whitelist:\n",
    "    whitelist_domains.append(extract_subdomain_and_domain(url))\n",
    "    whitelist_slds.append(extract_sld(url))\n",
    "typosquat = []\n",
    "for url in whitelist_domains:\n",
    "    fuzz = dnstwist.DomainFuzz(url)\n",
    "    fuzz.generate()\n",
    "    typosquat.extend([x['domain-name'] for x in fuzz.domains])\n",
    "    \n",
    "#Lookups in sets are much more efficient\n",
    "typosquat = set(typosquat)\n",
    "\n",
    "#Delete the original whitelisted domains from the blacklist set\n",
    "typosquat.difference_update(whitelist_domains)\n",
    "\n",
    "print('Number of typosquatted urls generated: ', len(typosquat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototype 4\n",
    "- included LD == 1 for further checks\n",
    "- added new helper function: exceeds_ked_check(), extract_chars()\n",
    "- improved efficiency, removed redundant/repetitive codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input - 1 URL\n",
    "# Output - True / False\n",
    "\n",
    "# e.g contains_special_characters(wikipedia.org) \n",
    "# expected output False\n",
    "\n",
    "# e.g contains_special_characters(wikipedi@.org) \n",
    "# expected output True\n",
    "\n",
    "def contains_special_characters(url):\n",
    "    # regex checks if the url is in the correct format\n",
    "    #(e.g. two periods (\".\") side-by-side is an invalid format)\n",
    "    return not re.match(\"^((?!-)[A-Za-z0-9-]{1,}(?<!-)\\.)+[A-Za-z0-9]{1,}$\", url)\n",
    "\n",
    "\n",
    "# Helper function to find out if the first TLD is more common than the second TLD\n",
    "\n",
    "# Input - 2 TLDs\n",
    "# Output - True / False\n",
    "\n",
    "# e.g exact_tld_swap(com, org) \n",
    "# expected output True\n",
    "\n",
    "# e.g exact_tld_swap(org, com) \n",
    "# expected output False\n",
    "\n",
    "def tld_more_common(tld1, tld2):    \n",
    "    tld1_index = 10\n",
    "    tld2_index = 10\n",
    "    if tld1 in tlds:\n",
    "        tld1_index = tlds.index(tld1)\n",
    "    if tld2 in tlds:\n",
    "        tld2_index = tlds.index(tld2)\n",
    "    if tld2_index > tld1_index:\n",
    "        return True\n",
    "    elif tld2_index < tld1_index:\n",
    "        return False\n",
    "\n",
    "\n",
    "# Helper function to check if \"wrong\" character is within KED of any of the other supplied characters\n",
    "\n",
    "# Input - 4 characters\n",
    "# Output - False\n",
    "\n",
    "# e.g. exceeds_ked_check(\"i\", \"i\", \"k\", \"i\")\n",
    "# expected output True\n",
    "\n",
    "# e.g. exceeds_ked_check(\"p\", \"d\", \"e\", \"m\")\n",
    "# expected output False\n",
    "    \n",
    "def exceeds_ked_check(left, right, correct, wrong):\n",
    "    result = True\n",
    "    \n",
    "    # if the wrong/extra char is at the start of the url, the left will most likely be empty\n",
    "    if left != \"\":\n",
    "        if euclidean_distance(left, wrong) < ked_boundary:\n",
    "            result = False\n",
    "    \n",
    "    # if the wrong/extra char is at the end of the url, the right will most likely be empty\n",
    "    if right != \"\":\n",
    "        if euclidean_distance(right, wrong) < ked_boundary:\n",
    "            result = False\n",
    "    \n",
    "    # if the error is an extra char, there wont be a \"correct\" char\n",
    "    if correct != \"\":\n",
    "        if euclidean_distance(correct, wrong) < ked_boundary:\n",
    "            result = False\n",
    "        \n",
    "    return result\n",
    "    \n",
    "# Helper function to extract wrong chars, correct chars, and chars on the left and right into a tuple of lists for easier comparison\n",
    "\n",
    "# Input - SequenceMatcher object, indexes of chars to be compared\n",
    "# Output - tuple of list    \n",
    "def extract_chars(seqm, a0, a1, b0, b1):\n",
    "    # list holding the chars on the left and right; index 0: left char, index 1: right char\n",
    "    left_right = []\n",
    "    \n",
    "    if len(seqm.a[a0:a1]) == 2:\n",
    "        left_right.append(seqm.a[a0-1:a1-2])\n",
    "        left_right.append(seqm.a[a0+2:a1+1])\n",
    "    else:\n",
    "        left_right.append(seqm.a[a0-1:a1-1])\n",
    "        left_right.append(seqm.a[a0+1:a1+1])\n",
    "    \n",
    "    # tuple consisting of 3 lists\n",
    "    # first list: wrong/extra char(s)\n",
    "    # second list: correct char(s) (may be empty if opcode == delete)\n",
    "    # third list: chars on the left and right\n",
    "    result = ([char for char in seqm.a[a0:a1]], [char for char in seqm.b[b0:b1]], left_right)\n",
    "    return result   \n",
    "\n",
    "    \n",
    "# Helper function to perform various euclidean distance checks based off of the lengths of both URLs\n",
    "\n",
    "# Input - 2 URLs\n",
    "# Output - dictionary of results\n",
    "\n",
    "# e.g. edit_check(\"w1k1pedia.org\", \"wikipedia.org\")\n",
    "# expected output: {'result': 1, 'reasons_typosquat': [\"'1' key and 'i' key are too far apart\", \"'1' key and 'i' key are too far apart\"], 'reasons_typo': []}\n",
    "def edit_check(sus_url, legit_url):\n",
    "    result = {\"result\": 0, \"reasons_typosquat\": [], \"reasons_typo\": []}\n",
    "    \n",
    "    # retrieving length of both URLs\n",
    "    sus_len = len(sus_url)\n",
    "    legit_len = len(legit_url)\n",
    "    \n",
    "    # retrieving levenshtein distance of both urls\n",
    "    ld = ls.levenshtein(sus_url, legit_url)\n",
    "    \n",
    "    # if the suspicious is only missing characters, then it's highly likely a typo than a typosquat\n",
    "    if sus_len == legit_len - 1 and ld == 1 or sus_len == legit_len - 2 and ld == 2:\n",
    "        result[\"reasons_typo\"].append(\"Only missing 1-2 characters\")\n",
    "        return result\n",
    "    \n",
    "    # checking if side-by-side swap\n",
    "    if sus_len == legit_len and ld == 2:\n",
    "        for index in range(sus_len):\n",
    "            if sus_url[index] != legit_url[index] and sus_url[index+1] != None:\n",
    "                if sus_url[index+1] == legit_url[index] and sus_url[index] == legit_url[index+1]:\n",
    "                    if allow_transposition:\n",
    "                        result[\"reasons_typo\"].append(\"Characters are just swapped in-place\")\n",
    "                        return result\n",
    "                    else:\n",
    "                        result[\"result\"] = 1\n",
    "                        result[\"reasons_typosquat\"].append(\"Characters were swapped\")\n",
    "                        return result\n",
    "    \n",
    "    \n",
    "    # creating an object that compares both urls\n",
    "    seqm = dl.SequenceMatcher(None, sus_url, legit_url)\n",
    "\n",
    "    # get_opcodes() gets the \"differences\" between each url, or the steps required for first url to match second url\n",
    "    for opcode, a0, a1, b0, b1 in seqm.get_opcodes():\n",
    "        # a : first url\n",
    "        # b : second url\n",
    "        # a0, a1 | b0, b1 : index range holding the characters being compared\n",
    "        # opcode : \"equal\", \"insert\", \"delete\", \"replace\" -- indicates the action require to turn a to b\n",
    "\n",
    "        # extracting chars to be compared\n",
    "        # chars_tuple[0] : list of wrong/extra char(s)\n",
    "        # chars_tuple[1] : list of correct char(s)\n",
    "        # chars_tuple[2] : list of chars on the left and right\n",
    "        chars_tuple = extract_chars(seqm, a0, a1, b0, b1)\n",
    "\n",
    "        # if a character has to be deleted, means it's an extra character\n",
    "        if opcode == 'delete':\n",
    "            for extra_char in chars_tuple[0]:\n",
    "                if exceeds_ked_check(chars_tuple[2][0], chars_tuple[2][1], \"\", extra_char):\n",
    "                    result[\"result\"] = 1\n",
    "                    result[\"reasons_typosquat\"].append(\"Extra character ('{}') exceeds keyboard euclidean distance boundary\".format(extra_char))\n",
    "                    return result\n",
    "\n",
    "        elif opcode == 'replace':\n",
    "            for wrong_char in chars_tuple[0]:\n",
    "                exceeds = True\n",
    "                if not exceeds_ked_check(chars_tuple[2][0], chars_tuple[2][1], chars_tuple[1][0], wrong_char):\n",
    "                    exceeds = False\n",
    "                if len(chars_tuple[1]) == 2:\n",
    "                    if not exceeds_ked_check(chars_tuple[2][0], chars_tuple[2][1], chars_tuple[1][1], wrong_char):\n",
    "                        exceeds = False\n",
    "\n",
    "                if exceeds:\n",
    "                    result[\"result\"] = 1\n",
    "                    result[\"reasons_typosquat\"].append(\"Wrong/Extra character ('{}') exceeds keyboard euclidean distance boundary\".format(wrong_char))\n",
    "                    return result\n",
    "                        \n",
    "    result[\"reasons_typo\"].append(\"Wrong/Extra characters within keyboard euclidean distance boundary\")\n",
    "        \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_typo(sus_original_url, legit_original_url):\n",
    "    # cleaning original_url\n",
    "    legit_url = extract_subdomain_and_domain(legit_original_url)\n",
    "    sus_url = extract_subdomain_and_domain(sus_original_url)\n",
    "    \n",
    "    # preparing \n",
    "    result = {\"legitimate_url\": legit_original_url, \"legitimate_cleaned_url\": legit_url, \"suspicious_url\": sus_original_url, \"suspicious_cleaned_url\": sus_url, \"result\": 0, \"reasons_typosquat\": [], \"reasons_typo\": []}\n",
    "    \n",
    "    # extracting SLD from both URLs\n",
    "    sus_sld = extract_sld(sus_url)\n",
    "    legit_sld = extract_sld(legit_url)\n",
    "    \n",
    "    # extracting TLD from both URLs\n",
    "    sus_tld = extract_tld(sus_url)\n",
    "    legit_tld = extract_tld(legit_url)\n",
    "    \n",
    "    # retrieving length of both URLs\n",
    "    sus_len = len(sus_url)\n",
    "    legit_len = len(legit_url)\n",
    "    \n",
    "    # check for illegal special characters\n",
    "    if contains_special_characters(sus_url):\n",
    "        result[\"result\"] = 1\n",
    "        result[\"reasons_typosquat\"].append(\"Illegal characters found in url\")\n",
    "        return result\n",
    "    \n",
    "    # check for exact TLD swap\n",
    "    if sus_sld == legit_sld and sus_tld != legit_tld:\n",
    "        if tld_more_common(sus_tld, legit_tld):\n",
    "            result[\"reasons_typo\"].append(\"TLD is more common\")\n",
    "            return result\n",
    "        else:\n",
    "            result[\"result\"] = 1\n",
    "            result[\"reasons_typosquat\"].append(\"TLD is less common\")\n",
    "            return result\n",
    "    \n",
    "    # check edit distance\n",
    "    ld = ls.levenshtein(sus_url, legit_url)\n",
    "    \n",
    "    # if only 1 or 2 edits\n",
    "    if ld == 1 or ld == 2:\n",
    "        res = edit_check(sus_url, legit_url)\n",
    "        \n",
    "        result[\"reasons_typosquat\"].extend(res[\"reasons_typosquat\"])\n",
    "        result[\"reasons_typo\"].extend(res[\"reasons_typo\"])\n",
    "        \n",
    "        if res[\"result\"] == 1:\n",
    "            result[\"result\"] = 1\n",
    "            return result\n",
    "    \n",
    "    # if 3 or more edits\n",
    "    else:\n",
    "        result[\"result\"] = \"Inconclusive\"\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'legitimate_url': 'bankofsingapore.com',\n",
       " 'legitimate_cleaned_url': 'bankofsingapore.com',\n",
       " 'suspicious_url': 'bankofsing4p0re.com',\n",
       " 'suspicious_cleaned_url': 'bankofsing4p0re.com',\n",
       " 'result': 1,\n",
       " 'reasons_typosquat': [\"Wrong/Extra character ('4') exceeds keyboard euclidean distance boundary\"],\n",
       " 'reasons_typo': ['Wrong/Extra characters within keyboard euclidean distance boundary']}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_typo(\"bankofsing4p0re.com\", \"bankofsingapore.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>legitimate_url</th>\n",
       "      <th>legitimate_cleaned_url</th>\n",
       "      <th>suspicious_url</th>\n",
       "      <th>suspicious_cleaned_url</th>\n",
       "      <th>result</th>\n",
       "      <th>reasons_typosquat</th>\n",
       "      <th>reasons_typo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.wikipedia.org/</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>https://www.wikipedi@.org/</td>\n",
       "      <td>wikipedi!.org</td>\n",
       "      <td>1</td>\n",
       "      <td>[Illegal characters found in url]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.wikipedia.org/</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>http://www.wikipedia.com/</td>\n",
       "      <td>wikipedia.com</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[TLD is more common]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://wikipedia.org/</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>https://wikipedia.br/</td>\n",
       "      <td>wikipedia.br</td>\n",
       "      <td>1</td>\n",
       "      <td>[TLD is less common]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://wikipedia.org/</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>http://wikipedi.org/</td>\n",
       "      <td>wikipedi.org</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Only missing 1-2 characters]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.abc.wikipedia.org/</td>\n",
       "      <td>abc.wikipedia.org</td>\n",
       "      <td>http://www.abc.kipedia.org/</td>\n",
       "      <td>abc.kipedia.org</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Only missing 1-2 characters]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.wikipedia.org/home.html</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>https://www.wikipediabb.org/home.html</td>\n",
       "      <td>wikipediabb.org</td>\n",
       "      <td>1</td>\n",
       "      <td>[Extra character ('b') exceeds keyboard euclidean distance boundary]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>m.wikipedia.org</td>\n",
       "      <td>m.wikipedia.org</td>\n",
       "      <td>m.wwikipediac.org</td>\n",
       "      <td>m.wwikipediac.org</td>\n",
       "      <td>1</td>\n",
       "      <td>[Extra character ('c') exceeds keyboard euclidean distance boundary]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>www.wikipedia.org/home.aspx</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>www.wikipedia.bvg/home.aspx</td>\n",
       "      <td>wikipedia.bvg</td>\n",
       "      <td>1</td>\n",
       "      <td>[TLD is less common]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://m.wikipedia.org</td>\n",
       "      <td>m.wikipedia.org</td>\n",
       "      <td>https://m.wikipemnia.org</td>\n",
       "      <td>m.wikipemnia.org</td>\n",
       "      <td>1</td>\n",
       "      <td>[Wrong/Extra character ('m') exceeds keyboard euclidean distance boundary]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>www.wikipedia.org</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>www.wbipedia.org</td>\n",
       "      <td>wbipedia.org</td>\n",
       "      <td>1</td>\n",
       "      <td>[Wrong/Extra character ('b') exceeds keyboard euclidean distance boundary]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://abc.wikipedia.org/</td>\n",
       "      <td>abc.wikipedia.org</td>\n",
       "      <td>https://abc.wiklped1o.0rg/</td>\n",
       "      <td>abc.wiklped1o.0rg</td>\n",
       "      <td>Inconclusive</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>www.wikipedia.org/about.php</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>www.wikiped1a.org/about.php</td>\n",
       "      <td>wikiped1a.org</td>\n",
       "      <td>1</td>\n",
       "      <td>[Wrong/Extra character ('1') exceeds keyboard euclidean distance boundary]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>https://www.wikipedia.org/</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>https://www.wikipediaa.org/</td>\n",
       "      <td>wikipediaa.org</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Wrong/Extra characters within keyboard euclidean distance boundary]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>wlklpedla.org</td>\n",
       "      <td>wlklpedla.org</td>\n",
       "      <td>Inconclusive</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         legitimate_url legitimate_cleaned_url  \\\n",
       "0            https://www.wikipedia.org/          wikipedia.org   \n",
       "1             http://www.wikipedia.org/          wikipedia.org   \n",
       "2                https://wikipedia.org/          wikipedia.org   \n",
       "3                 http://wikipedia.org/          wikipedia.org   \n",
       "4         http://www.abc.wikipedia.org/      abc.wikipedia.org   \n",
       "5   https://www.wikipedia.org/home.html          wikipedia.org   \n",
       "6                       m.wikipedia.org        m.wikipedia.org   \n",
       "7           www.wikipedia.org/home.aspx          wikipedia.org   \n",
       "8               https://m.wikipedia.org        m.wikipedia.org   \n",
       "9                     www.wikipedia.org          wikipedia.org   \n",
       "10           https://abc.wikipedia.org/      abc.wikipedia.org   \n",
       "11          www.wikipedia.org/about.php          wikipedia.org   \n",
       "12           https://www.wikipedia.org/          wikipedia.org   \n",
       "13                        wikipedia.org          wikipedia.org   \n",
       "\n",
       "                           suspicious_url suspicious_cleaned_url  \\\n",
       "0              https://www.wikipedi@.org/          wikipedi!.org   \n",
       "1               http://www.wikipedia.com/          wikipedia.com   \n",
       "2                   https://wikipedia.br/           wikipedia.br   \n",
       "3                    http://wikipedi.org/           wikipedi.org   \n",
       "4             http://www.abc.kipedia.org/        abc.kipedia.org   \n",
       "5   https://www.wikipediabb.org/home.html        wikipediabb.org   \n",
       "6                       m.wwikipediac.org      m.wwikipediac.org   \n",
       "7             www.wikipedia.bvg/home.aspx          wikipedia.bvg   \n",
       "8                https://m.wikipemnia.org       m.wikipemnia.org   \n",
       "9                        www.wbipedia.org           wbipedia.org   \n",
       "10             https://abc.wiklped1o.0rg/      abc.wiklped1o.0rg   \n",
       "11            www.wikiped1a.org/about.php          wikiped1a.org   \n",
       "12            https://www.wikipediaa.org/         wikipediaa.org   \n",
       "13                          wlklpedla.org          wlklpedla.org   \n",
       "\n",
       "          result  \\\n",
       "0              1   \n",
       "1              0   \n",
       "2              1   \n",
       "3              0   \n",
       "4              0   \n",
       "5              1   \n",
       "6              1   \n",
       "7              1   \n",
       "8              1   \n",
       "9              1   \n",
       "10  Inconclusive   \n",
       "11             1   \n",
       "12             0   \n",
       "13  Inconclusive   \n",
       "\n",
       "                                                             reasons_typosquat  \\\n",
       "0                                            [Illegal characters found in url]   \n",
       "1                                                                           []   \n",
       "2                                                         [TLD is less common]   \n",
       "3                                                                           []   \n",
       "4                                                                           []   \n",
       "5         [Extra character ('b') exceeds keyboard euclidean distance boundary]   \n",
       "6         [Extra character ('c') exceeds keyboard euclidean distance boundary]   \n",
       "7                                                         [TLD is less common]   \n",
       "8   [Wrong/Extra character ('m') exceeds keyboard euclidean distance boundary]   \n",
       "9   [Wrong/Extra character ('b') exceeds keyboard euclidean distance boundary]   \n",
       "10                                                                          []   \n",
       "11  [Wrong/Extra character ('1') exceeds keyboard euclidean distance boundary]   \n",
       "12                                                                          []   \n",
       "13                                                                          []   \n",
       "\n",
       "                                                            reasons_typo  \n",
       "0                                                                     []  \n",
       "1                                                   [TLD is more common]  \n",
       "2                                                                     []  \n",
       "3                                          [Only missing 1-2 characters]  \n",
       "4                                          [Only missing 1-2 characters]  \n",
       "5                                                                     []  \n",
       "6                                                                     []  \n",
       "7                                                                     []  \n",
       "8                                                                     []  \n",
       "9                                                                     []  \n",
       "10                                                                    []  \n",
       "11                                                                    []  \n",
       "12  [Wrong/Extra characters within keyboard euclidean distance boundary]  \n",
       "13                                                                    []  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "legit_urls = [\"https://www.wikipedia.org/\", \"http://www.wikipedia.org/\", \"https://wikipedia.org/\", \"http://wikipedia.org/\", \"http://www.abc.wikipedia.org/\", \"https://www.wikipedia.org/home.html\", \"m.wikipedia.org\", \"www.wikipedia.org/home.aspx\", \"https://m.wikipedia.org\", \"www.wikipedia.org\", \"https://abc.wikipedia.org/\", \"www.wikipedia.org/about.php\", \"https://www.wikipedia.org/\", \"wikipedia.org\"]\n",
    "sus_urls = [\"https://www.wikipedi@.org/\", \"http://www.wikipedia.com/\", \"https://wikipedia.br/\", \"http://wikipedi.org/\", \"http://www.abc.kipedia.org/\", \"https://www.wikipediabb.org/home.html\", \"m.wwikipediac.org\", \"www.wikipedia.bvg/home.aspx\", \"https://m.wikipemnia.org\", \"www.wbipedia.org\", \"https://abc.wiklped1o.0rg/\", \"www.wikiped1a.org/about.php\", \"https://www.wikipediaa.org/\", \"wlklpedla.org\"] \n",
    "test_results = []\n",
    "\n",
    "for index in range(len(legit_urls)):\n",
    "    test_results.append(is_typo(sus_urls[index], legit_urls[index]))\n",
    "    \n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df = pd.DataFrame(test_results)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
