{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements \n",
    "<ul>\n",
    "    <li>Your function should account for keyboard distance for all characters (even special characters)</li>\n",
    "    <li>Your function should account for levenshtein distance as well. If there are more typos, then the score should lean more towards 1 (since it is unlikely for a user to make so many typos)</li>\n",
    "    <li>Think about if it is more likely to make a horizontal typo vs a vertical typo, you may want to assign a weight to differentiate the typos</li>\n",
    "    <li>Think about the case when the strings have different lengths and how you should handle it</li>\n",
    "    <li>Think about if it is necessary to distinguish if the character is already very far away (e.g wikip9dia.org vs wikip0dia.org), both are most likely typosquats, is there a need for a different score? How many keyboard characters away then should I consider it to be not a typo vs not typo?</li>\n",
    "    <li>Try to think of any other conditions / requirements that I may have missed out, and feel free to suggest any</li>\n",
    "</ul>\n",
    "\n",
    "what about swapped letters, one-too-many letters\n",
    "\n",
    "numbers above qwertyuiop are possible typos, but some may be intended typosquats (i.e. o -> 0; E -> 3 ?)\n",
    "\n",
    "what about when a user presses 2 keys on accident? e.g. wikoipedia -> presses \"o\" and \"k\" when trying to press \"k\"\n",
    "\n",
    "are special chars/homoglyphs legal in the url box?\n",
    "\n",
    "what if they miss a letter?\n",
    "\n",
    "\n",
    "[python-Levenshtein PyPI](https://pypi.org/project/python-Levenshtein/)\n",
    "\n",
    "[euclidean distance using numpy (stack overflow)](https://stackoverflow.com/questions/1401712/how-can-the-euclidean-distance-be-calculated-with-numpy) (may help make calculating ED more efficient?)\n",
    "\n",
    "[top 10 most common TLDs](https://www.statista.com/statistics/265677/number-of-internet-top-level-domains-worldwide/)\n",
    "\n",
    "[domain name regex](https://medium.com/@vaghasiyaharryk/how-to-validate-a-domain-name-using-regular-expression-9ab484a1b430)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import *\n",
    "import dnstwist\n",
    "from tldextract import extract\n",
    "import pylev as ls\n",
    "# import Levenshtein as ls\n",
    "import numpy as np\n",
    "import difflib as dl\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.4142135623730951\n"
     ]
    }
   ],
   "source": [
    "keyboard_cartesian = {\n",
    "                        \"1\": {\"y\": -1, \"x\": 0},\n",
    "                        \"2\": {\"y\": -1, \"x\": 1},\n",
    "                        \"3\": {\"y\": -1, \"x\": 2},\n",
    "                        \"4\": {\"y\": -1, \"x\": 3},\n",
    "                        \"5\": {\"y\": -1, \"x\": 4},\n",
    "                        \"6\": {\"y\": -1, \"x\": 5},\n",
    "                        \"7\": {\"y\": -1, \"x\": 6},\n",
    "                        \"8\": {\"y\": -1, \"x\": 7},\n",
    "                        \"9\": {\"y\": -1, \"x\": 8},\n",
    "                        \"0\": {\"y\": -1, \"x\": 9},\n",
    "                        \"-\": {\"y\": -1, \"x\": 10},\n",
    "                        \"q\": {\"y\": 0, \"x\": 0},\n",
    "                        \"w\": {\"y\": 0, \"x\": 1},\n",
    "                        \"e\": {\"y\": 0, \"x\": 2},\n",
    "                        \"r\": {\"y\": 0, \"x\": 3},\n",
    "                        \"t\": {\"y\": 0, \"x\": 4},\n",
    "                        \"y\": {\"y\": 0, \"x\": 5},\n",
    "                        \"u\": {\"y\": 0, \"x\": 6},\n",
    "                        \"i\": {\"y\": 0, \"x\": 7},\n",
    "                        \"o\": {\"y\": 0, \"x\": 8},\n",
    "                        \"p\": {\"y\": 0, \"x\": 9},\n",
    "                        \"a\": {\"y\": 1, \"x\": 0},\n",
    "                        \"s\": {\"y\": 1, \"x\": 1},\n",
    "                        \"d\": {\"y\": 1, \"x\": 2},\n",
    "                        \"f\": {\"y\": 1, \"x\": 3},\n",
    "                        \"g\": {\"y\": 1, \"x\": 4},\n",
    "                        \"h\": {\"y\": 1, \"x\": 5},\n",
    "                        \"j\": {\"y\": 1, \"x\": 6},\n",
    "                        \"k\": {\"y\": 1, \"x\": 7},\n",
    "                        \"l\": {\"y\": 1, \"x\": 8},\n",
    "                        \";\": {\"y\": 2, \"x\": 9},\n",
    "                        \"'\": {\"y\": 2, \"x\": 10},\n",
    "                        \"z\": {\"y\": 2, \"x\": 0},\n",
    "                        \"x\": {\"y\": 2, \"x\": 1},\n",
    "                        \"c\": {\"y\": 2, \"x\": 2},\n",
    "                        \"v\": {\"y\": 2, \"x\": 3},\n",
    "                        \"b\": {\"y\": 2, \"x\": 4},\n",
    "                        \"n\": {\"y\": 2, \"x\": 5},\n",
    "                        \"m\": {\"y\": 2, \"x\": 6},\n",
    "                        \",\": {\"y\": 2, \"x\": 7},\n",
    "                        \".\": {\"y\": 2, \"x\": 8},\n",
    "                        \"/\": {\"y\": 2, \"x\": 9}                   \n",
    "                     }\n",
    "\n",
    "def euclidean_distance(a,b):\n",
    "    X = (keyboard_cartesian[a]['x']-keyboard_cartesian[b]['x'])**2\n",
    "    Y = (keyboard_cartesian[a]['y']-keyboard_cartesian[b]['y'])**2\n",
    "    return sqrt(X+Y)\n",
    "\n",
    "print(euclidean_distance('q', 'w'))\n",
    "print(euclidean_distance('q', 's'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Try\n",
    "simply checking against Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "legit = \"wikipedia.org\"\n",
    "typo = \"wiiipedia.org\" \n",
    "typosqt = \"wikiped1a.org\"\n",
    "\n",
    "def typo_check(url):\n",
    "    for i in range(len(legit)):\n",
    "        if legit[i] != url[i]:\n",
    "            result = euclidean_distance(legit[i], url[i])\n",
    "            if result >= 1.5:\n",
    "                # typosquat\n",
    "                return 1\n",
    "            else:\n",
    "                # typo\n",
    "                return 0\n",
    "\n",
    "print(typo_check(typosqt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T : URL being tested <br>\n",
    "L : Legit URL <br>\n",
    "LD : levenshtein distance <br>\n",
    "ED : euclidean distance <br>\n",
    "\n",
    "Assumptions: <br>\n",
    "<ul >\n",
    "    <li>Given the suspicious URL (S), its genuine URL (G) is known</li>\n",
    "    <li>S has an edit distance of at least 1</li>\n",
    "    <li>There is no reason to press shift; urls are not case sensitive, and the only legal special character is hyphen, which does not require shift</li>\n",
    "    <li>Hyphens are only allowed in between characters in domain names (i.e. domain names cannot start/end with hyphens, and the top level domain cannot contain hyphen)</li>\n",
    "    <li>T is assumed to be a typosquat when it meets one of the following:\n",
    "        <ol>\n",
    "            <li>Contains illegal special characters</li>\n",
    "            <li>Has a Levenshtein distance of more than 1</li>\n",
    "            <li>Is shorter than len(G) - 1</li>\n",
    "            <li>Is longer than len(G) + 1</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "Then, checks:\n",
    "- if there are special characters in T, typosquat (in domain names: hyphens are allowed, underscores are not)\n",
    "- if length of T = length of L + 1, check how far away the extra letter is from the previous letter. was it fat-fingered?\n",
    "- if length of T = length of L, check LD. if > 2, definitely typosquat\n",
    "- if length of T = length of L AND LD <= 2, check ED. if any error has ED > 1.5, typosquat\n",
    "\n",
    "definitely typosquat:\n",
    "- special char present anywhere\n",
    "- len(T) > len(L) + 1\n",
    "- LD > 2\n",
    "- ED > 1.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions & Codes For Testing\n",
    "functions to extract TLD & SLD\n",
    "\n",
    "codes to generate various suspicious URLs to test based off of legit URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of typosquatted urls generated:  7900\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/44113335/extract-domain-from-url-in-python\n",
    "\n",
    "def replace_special_char(char):\n",
    "    flag = '\"!@#$%^&*()+?_=,<>'':\\\\'\n",
    "    flag_list = [char for char in flag]\n",
    "    if char.isalnum()==False and char in flag:\n",
    "        return 'Z'\n",
    "    return char\n",
    "    \n",
    "# Clean the string first. The extract python library cannot properly extract the domain from URLs with special characters\n",
    "\n",
    "# 1. make string lower case\n",
    "# 2. replace all flagged special characters with 'Z'\n",
    "# 3. extract the domain or TLD\n",
    "# 4. replace all 'Z' with '!'\n",
    "# 5. calculate edit distance\n",
    "\n",
    "def clean_string(url):\n",
    "    # First make the string lowercase\n",
    "    url = url.lower()\n",
    "\n",
    "    # ':' is a flagged character, but if it appears with http or https it is fine\n",
    "    url = url.replace('https://','')\n",
    "    url = url.replace('http://','')\n",
    "    return ''.join([replace_special_char(char) for char in url])\n",
    "    \n",
    "def extract_domain_and_tld(url):\n",
    "    url = clean_string(url)\n",
    "    tsd, td, tsu = extract(url)\n",
    "    final = td + '.' + tsu\n",
    "    return final.replace('Z','!')\n",
    "    \n",
    "def extract_tld(url):\n",
    "    url = clean_string(url)\n",
    "    tsd, td, tsu = extract(url)\n",
    "    return tsu.replace('Z','!')  \n",
    "\n",
    "def extract_sld(url):\n",
    "    url = clean_string(url)\n",
    "    tsd, td, tsu = extract(url)\n",
    "    return td.replace('Z','!')\n",
    "\n",
    "# Theres no need for this container to be mutable and tuples are faster\n",
    "# Index 0 - 9 = most popular to least popular\n",
    "tlds = (\"com\", \"ru\", \"org\", \"net\", \"in\", \"ir\", \"au\", \"uk\", \"de\", \"br\")\n",
    "\n",
    "whitelist = ['https://www.wikipedia.org/']\n",
    "\n",
    "\n",
    "whitelist_domains = []\n",
    "whitelist_slds = []\n",
    "for url in whitelist:\n",
    "    whitelist_domains.append(extract_domain_and_tld(url))\n",
    "    whitelist_slds.append(extract_sld(url))\n",
    "typosquat = []\n",
    "for url in whitelist_domains:\n",
    "    fuzz = dnstwist.DomainFuzz(url)\n",
    "    fuzz.generate()\n",
    "    typosquat.extend([x['domain-name'] for x in fuzz.domains])\n",
    "    \n",
    "#Lookups in sets are much more efficient\n",
    "typosquat = set(typosquat)\n",
    "\n",
    "#Delete the original whitelisted domains from the blacklist set\n",
    "typosquat.difference_update(whitelist_domains)\n",
    "\n",
    "print('Number of typosquatted urls generated: ', len(typosquat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bankofsingapore.com'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_domain_and_tld(\"forums.bankofsingapore.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia.org'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_domain_and_tld(\"wikipedia.org\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia!.org'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_domain_and_tld(\"wikipedia#.org\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia!.org'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_domain_and_tld(\"wikipedia@.org\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia!.org'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_domain_and_tld(\"wikipedia-.org\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia!.org'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_domain_and_tld(\"wikipedia:.org\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## is_typo(url) Function\n",
    "Code Flow:\n",
    "- checks if url has any special characters\n",
    "- checks LD of url against legit URL\n",
    "- checks the TLD of url agains legit URL\n",
    "- checks length of url against lenght of legit URL\n",
    "- checks the ED of any wrong char in url against corresponding char in legit URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'suspicious url': 'wikipedia.org', 'original url': 'wikipedia.org', 'result': 0, 'reasons_typosquat': [], 'reasons_typo': []}\n",
      "{'suspicious url': 'wikipedip.org', 'original url': 'wikipedia.org', 'result': 1, 'reasons_typosquat': [\"'p' key and 'a' key are too far apart\"], 'reasons_typo': ['Edit distance is only one']}\n",
      "{'suspicious url': 'wikipedia.com', 'original url': 'wikipedia.org', 'result': 1, 'reasons_typosquat': ['Edit distance more than 1', \"'c' key and 'o' key are too far apart\", \"'o' key and 'r' key are too far apart\", \"'m' key and 'g' key are too far apart\"], 'reasons_typo': ['TLD is more common']}\n"
     ]
    }
   ],
   "source": [
    "def is_typo(url):\n",
    "    l = whitelist_domains[0]\n",
    "    \n",
    "    result = {\"suspicious url\": url, \"original url\": l, \"result\": 0, \"reasons_typosquat\": [], \"reasons_typo\": []}\n",
    "    \n",
    "    url_sld = extract_sld(url)\n",
    "    l_sld = extract_sld(l)\n",
    "    \n",
    "    url_tld = extract_tld(url)\n",
    "    l_tld = extract_tld(l)\n",
    "    \n",
    "    url_len = len(url)\n",
    "    l_len = len(l)\n",
    "    \n",
    "    # checks if illegal special characters are present\n",
    "    if not re.match(\"^[^-][a-zA-Z0-9-]{1,}[^-][.]{1}[a-zA-Z0-9]{1,}$\", url):\n",
    "        result[\"result\"] = 1\n",
    "        result[\"reasons_typosquat\"].append(\"Illegal characters found in url\")\n",
    "    \n",
    "    # checks LD\n",
    "    if ls.levenshtein(url, l) > 1:\n",
    "        result[\"result\"] = 1\n",
    "        result[\"reasons_typosquat\"].append(\"Edit distance more than 1\")\n",
    "    elif ls.levenshtein(url, l) == 1:\n",
    "        result[\"reasons_typo\"].append(\"Edit distance is only one\")\n",
    "    \n",
    "    # checks TLD\n",
    "    if url_tld != l_tld:\n",
    "        url_index = 10\n",
    "        l_index = 10\n",
    "        if url_tld in tlds:\n",
    "            url_index = tlds.index(url_tld)\n",
    "        if l_tld in tlds:\n",
    "            l_index = tlds.index(l_tld)\n",
    "        if l_index > url_index:\n",
    "            result[\"reasons_typo\"].append(\"TLD is more common\")\n",
    "            \n",
    "     \n",
    "    # compares lengths\n",
    "    if url_len > l_len + 1:\n",
    "        result[\"result\"] = 1\n",
    "        result[\"reasons_typosquat\"].append(\"Too long\")\n",
    "    \n",
    "    elif url_len < l_len - 1:\n",
    "        result[\"result\"] = 1\n",
    "        result[\"reasons_typosquat\"].append(\"Too short\")\n",
    "    \n",
    "    elif url_len == l_len + 1:\n",
    "        for i in range(len(l)):\n",
    "            if url[i] != l[i]:\n",
    "                url_left = url[i - 1] if i != 0 else None # char on the left of the wrong/extra char\n",
    "                url_middle = url[i] # wrong/extra char\n",
    "                url_right = url[i + 1] if i + 1 < len(url) else None # char on the right of wrong/extra char          \n",
    "                \n",
    "                if url_left == None:\n",
    "                    if euclidean_distance(url_right, url_middle) > 1.5:\n",
    "                        result[\"result\"] = 1\n",
    "                        result[\"reasons_typosquat\"].append(\"Extra character too far from characters next to it\")\n",
    "                    else:\n",
    "                        result[\"reasons_typo\"].append(\"Extra character is within ED boundary\")\n",
    "                        \n",
    "                elif url_right == None:\n",
    "                    if euclidean_distance(url_left, url_middle) > 1.5:\n",
    "                        result[\"result\"] = 1\n",
    "                        result[\"reasons_typosquat\"].append(\"Extra character too far from characters next to it\")\n",
    "                    else:\n",
    "                        result[\"reasons_typo\"].append(\"Extra character is within ED boundary\")\n",
    "                else:\n",
    "                    if euclidean_distance(url_left, url_middle) > 1.5 and euclidean_distance(url_right, url_middle) > 1.5:\n",
    "                        result[\"result\"] = 1\n",
    "                        result[\"reasons_typosquat\"].append(\"Extra character too far from characters next to it\")\n",
    "                    else:\n",
    "                        result[\"reasons_typo\"].append(\"Extra character is within ED boundary\")\n",
    "                \n",
    "                # prevent the function from running on the rest of the string\n",
    "                break\n",
    "                \n",
    "    elif url_len == l_len:\n",
    "        for i in range(len(l)):\n",
    "            if url[i] != l[i]:\n",
    "                if euclidean_distance(url[i], l[i]) > 1.5:\n",
    "                    result[\"result\"] = 1\n",
    "                    result[\"reasons_typosquat\"].append(\"'{}' key and '{}' key are too far apart\".format(url[i], l[i]))\n",
    "                else:\n",
    "                        result[\"reasons_typo\"].append(\"Wrong character is within ED boundary\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(is_typo(\"wikipedia.org\"))\n",
    "print(is_typo(\"wikipedip.org\"))\n",
    "print(is_typo(\"wikipedia.com\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "<u>Potential Results</u>: <br>\n",
    "0: Typo <br>\n",
    "1: Typosquat\n",
    "\n",
    "<u>Test Cases</u>:\n",
    "1. Legit URL (Expected Result: 0)\n",
    "2. Substitute 1 char with SC (Expected Result: 1)\n",
    "3. Substitute 1 char with hyphen (Expected Result: 1)\n",
    "4. Substitute 1 char with wrong char, within ED boundary (Expected Result: 0)\n",
    "5. Substitute 2 char with wrong char, within ED boundary (Expected Result: 0)\n",
    "6. Substitute 3 char with wrong char, within ED boundary(Expected Result: 1)\n",
    "7. Append 1 char, within ED boundary of last char (Expected Result: 0)\n",
    "8. Append 1 char, exceeding ED boundary (Expected Result: 1)\n",
    "9. Append 2 char, within ED boundary(Expected Result: 1)\n",
    "10. Remove 1 char (Expected Result: 1)\n",
    "    \n",
    "<u>Conclusion/Notes</u>:\n",
    "- need to recheck for special characters, as they are not accounted for in the next checks after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL 0 : {'suspicious url': 'bankofsingapore.com', 'original url': 'wikipedia.org', 'result': 1, 'reasons_typosquat': ['Edit distance more than 1', 'Too long'], 'reasons_typo': ['TLD is more common']}\n",
      "\n",
      "URL 1 : {'suspicious url': 'bankofs!ngapore.com', 'original url': 'wikipedia.org', 'result': 1, 'reasons_typosquat': ['Illegal characters found in url', 'Edit distance more than 1', 'Too long'], 'reasons_typo': ['TLD is more common']}\n",
      "\n",
      "URL 2 : {'suspicious url': 'bankofsing-pore.com', 'original url': 'wikipedia.org', 'result': 1, 'reasons_typosquat': ['Edit distance more than 1', 'Too long'], 'reasons_typo': ['TLD is more common']}\n",
      "\n",
      "URL 3 : {'suspicious url': 'bankofsingaporr.com', 'original url': 'wikipedia.org', 'result': 1, 'reasons_typosquat': ['Edit distance more than 1', 'Too long'], 'reasons_typo': ['TLD is more common']}\n",
      "\n",
      "URL 4 : {'suspicious url': 'bankofsingapoer.com', 'original url': 'wikipedia.org', 'result': 1, 'reasons_typosquat': ['Edit distance more than 1', 'Too long'], 'reasons_typo': ['TLD is more common']}\n",
      "\n",
      "URL 5 : {'suspicious url': 'bankofsingapier.com', 'original url': 'wikipedia.org', 'result': 1, 'reasons_typosquat': ['Edit distance more than 1', 'Too long'], 'reasons_typo': ['TLD is more common']}\n",
      "\n",
      "URL 6 : {'suspicious url': 'bankofsingaporee.com', 'original url': 'wikipedia.org', 'result': 1, 'reasons_typosquat': ['Edit distance more than 1', 'Too long'], 'reasons_typo': ['TLD is more common']}\n",
      "\n",
      "URL 7 : {'suspicious url': 'bankofsingaporep.com', 'original url': 'wikipedia.org', 'result': 1, 'reasons_typosquat': ['Edit distance more than 1', 'Too long'], 'reasons_typo': ['TLD is more common']}\n",
      "\n",
      "URL 8 : {'suspicious url': 'bankofsingaporeee.com', 'original url': 'wikipedia.org', 'result': 1, 'reasons_typosquat': ['Edit distance more than 1', 'Too long'], 'reasons_typo': ['TLD is more common']}\n",
      "\n",
      "URL 9 : {'suspicious url': 'bankofsingapor.com', 'original url': 'wikipedia.org', 'result': 1, 'reasons_typosquat': ['Edit distance more than 1', 'Too long'], 'reasons_typo': ['TLD is more common']}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_cases = [\"bankofsingapore.com\", \n",
    "              \"bankofs!ngapore.com\", \n",
    "              \"bankofsing-pore.com\", \n",
    "              \"bankofsingaporr.com\", \n",
    "              \"bankofsingapoer.com\", \n",
    "              \"bankofsingapier.com\", \n",
    "              \"bankofsingaporee.com\", \n",
    "              \"bankofsingaporep.com\", \n",
    "              \"bankofsingaporeee.com\", \n",
    "              \"bankofsingapor.com\"]\n",
    "\n",
    "for i in range(len(test_cases)):\n",
    "    print(\"URL\", i, \":\", is_typo(test_cases[i]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototype 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'suspicious url': 'wikipedia.org',\n",
       " 'original url': 'wikipedia.org',\n",
       " 'result': 0,\n",
       " 'reasons_typosquat': [],\n",
       " 'reasons_typo': []}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_typo(sus_url, legit_url):\n",
    "    result = {\"suspicious url\": sus_url, \"original url\": legit_url, \"result\": 0, \"reasons_typosquat\": [], \"reasons_typo\": []}\n",
    "    \n",
    "    sus_sld = extract_sld(sus_url)\n",
    "    legit_sld = extract_sld(legit_url)\n",
    "    \n",
    "    sus_tld = extract_tld(sus_url)\n",
    "    legit_tld = extract_tld(legit_url)\n",
    "    \n",
    "    sus_len = len(sus_url)\n",
    "    legit_len = len(legit_url)\n",
    "    \n",
    "    # checks length\n",
    "    if legit_len + 1 < sus_len:\n",
    "        result[\"result\"] = 1\n",
    "        result[\"reasons_typosquat\"].append(\"Too long\")\n",
    "        return result\n",
    "    elif sus_len < legit_len - 1:\n",
    "        result[\"result\"] = 1\n",
    "        result[\"reasons_typosquat\"].append(\"Too short\")\n",
    "        return result\n",
    "    \n",
    "    # checks if illegal special characters are present\n",
    "    if not re.match(\"^((?!-)[A-Za-z0–9-]{1,}(?<!-)\\.)+[A-Za-z0-9]{1,}$\", sus_url):\n",
    "        result[\"result\"] = 1\n",
    "        result[\"reasons_typosquat\"].append(\"Illegal characters found in url\")\n",
    "        return result\n",
    "    \n",
    "    # checks TLD; WIP\n",
    "    if sus_tld != legit_tld:\n",
    "        sus_index = 10\n",
    "        legit_index = 10\n",
    "        if sus_tld in tlds:\n",
    "            sus_index = tlds.index(sus_tld)\n",
    "        if legit_tld in tlds:\n",
    "            legit_index = tlds.index(legit_tld)\n",
    "        if legit_index > sus_index:\n",
    "            result[\"reasons_typo\"].append(\"TLD is more common\")\n",
    "            \n",
    "            # reassign values so next checks will only be on sld, as tld mistakes have already been ruled out\n",
    "            sus_url = sus_sld\n",
    "            legit_url = legit_sld\n",
    "            sus_len = len(sus_url)\n",
    "            legit_len = len(legit_url)\n",
    "    \n",
    "    \n",
    "    # levenshtein distance check\n",
    "    if ls.levenshtein(sus_url, legit_url) > 1:\n",
    "        result[\"result\"] = 1\n",
    "        result[\"reasons_typosquat\"].append(\"Edit distance more than 1\")\n",
    "        return result\n",
    "    else:\n",
    "        # length checks\n",
    "        if sus_len == legit_len:\n",
    "            for i in range(len(legit_url)):\n",
    "                if sus_url[i] != legit_url[i]:\n",
    "                    if euclidean_distance(sus_url[i], legit_url[i]) > 1.5:\n",
    "                        result[\"result\"] = 1\n",
    "                        result[\"reasons_typosquat\"].append(\"'{}' key and '{}' key are too far apart\".format(sus_url[i], legit_url[i]))\n",
    "                    else:\n",
    "                            result[\"reasons_typo\"].append(\"Wrong character is within ED boundary\")\n",
    "        \n",
    "        elif sus_len == legit_len + 1:\n",
    "            for i in range(len(legit_url)):\n",
    "                if sus_url[i] != legit_url[i]:\n",
    "                    sus_left = sus_url[i - 1] if i != 0 else None # char on the left of the wrong/extra char\n",
    "                    sus_middle = sus_url[i] # wrong/extra char\n",
    "                    sus_right = sus_url[i + 1] if i + 1 < len(url) else None # char on the right of wrong/extra char          \n",
    "\n",
    "                    if sus_left == None:\n",
    "                        if euclidean_distance(sus_right, sus_middle) > 1.5:\n",
    "                            result[\"result\"] = 1\n",
    "                            result[\"reasons_typosquat\"].append(\"Extra character too far from characters next to it\")\n",
    "                        else:\n",
    "                            result[\"reasons_typo\"].append(\"Extra character is within ED boundary\")\n",
    "\n",
    "                    elif sus_right == None:\n",
    "                        if euclidean_distance(sus_left, sus_middle) > 1.5:\n",
    "                            result[\"result\"] = 1\n",
    "                            result[\"reasons_typosquat\"].append(\"Extra character too far from characters next to it\")\n",
    "                        else:\n",
    "                            result[\"reasons_typo\"].append(\"Extra character is within ED boundary\")\n",
    "                    else:\n",
    "                        if euclidean_distance(sus_left, sus_middle) > 1.5 and euclidean_distance(sus_right, sus_middle) > 1.5:\n",
    "                            result[\"result\"] = 1\n",
    "                            result[\"reasons_typosquat\"].append(\"Extra character too far from characters next to it\")\n",
    "                        else:\n",
    "                            result[\"reasons_typo\"].append(\"Extra character is within ED boundary\")\n",
    "\n",
    "                    # prevent the function from running on the rest of the string\n",
    "                    break\n",
    "        \n",
    "        elif sus_len == legit_len - 1:\n",
    "            result[\"reasons_typo\"].append(\"Only missing 1 character\")\n",
    "            \n",
    "        if len(result[\"reasons_typo\"]) > 1:\n",
    "            result[\"result\"] = 1\n",
    "            result[\"reasons_typosquat\"].append(\"Too many typos\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "legit_url = whitelist_domains[0]\n",
    "\n",
    "# original url\n",
    "is_typo(\"wikipedia.org\", legit_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'suspicious url': 'wikipediaaaaa.org',\n",
       " 'original url': 'wikipedia.org',\n",
       " 'result': 1,\n",
       " 'reasons_typosquat': ['Too long'],\n",
       " 'reasons_typo': []}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# too long\n",
    "is_typo(\"wikipediaaaaa.org\", legit_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'suspicious url': 'wikipe.org',\n",
       " 'original url': 'wikipedia.org',\n",
       " 'result': 1,\n",
       " 'reasons_typosquat': ['Too short'],\n",
       " 'reasons_typo': []}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# too short\n",
    "is_typo(\"wikipe.org\", legit_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'suspicious url': 'wikipedi@.org',\n",
       " 'original url': 'wikipedia.org',\n",
       " 'result': 1,\n",
       " 'reasons_typosquat': ['Illegal characters found in url'],\n",
       " 'reasons_typo': []}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with special character\n",
    "is_typo(\"wikipedi@.org\", legit_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'suspicious url': 'wikipedia.com',\n",
       " 'original url': 'wikipedia.org',\n",
       " 'result': 0,\n",
       " 'reasons_typosquat': [],\n",
       " 'reasons_typo': ['TLD is more common']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exact TLD swap\n",
    "is_typo(\"wikipedia.com\", legit_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'suspicious url': 'wikipedis.com',\n",
       " 'original url': 'wikipedia.org',\n",
       " 'result': 1,\n",
       " 'reasons_typosquat': ['Too many typos'],\n",
       " 'reasons_typo': ['TLD is more common',\n",
       "  'Wrong character is within ED boundary']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 typos\n",
    "is_typo(\"wikipedis.com\", legit_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'suspicious url': 'wikipedai.org',\n",
       " 'original url': 'wikipedia.org',\n",
       " 'result': 1,\n",
       " 'reasons_typosquat': ['Edit distance more than 1'],\n",
       " 'reasons_typo': []}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LD = 2\n",
    "is_typo(\"wikipedai.org\", legit_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'suspicious url': 'wikipedih.org',\n",
       " 'original url': 'wikipedia.org',\n",
       " 'result': 1,\n",
       " 'reasons_typosquat': [\"'h' key and 'a' key are too far apart\"],\n",
       " 'reasons_typo': []}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lengths are equal\n",
    "is_typo(\"wikipedih.org\", legit_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'suspicious url': 'wikipediap.org',\n",
       " 'original url': 'wikipedia.org',\n",
       " 'result': 1,\n",
       " 'reasons_typosquat': ['Extra character too far from characters next to it'],\n",
       " 'reasons_typo': []}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one character too long\n",
    "is_typo(\"wikipediap.org\", legit_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototype 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Input - 1 URL\n",
    "# Output - True / False\n",
    "\n",
    "def contains_special_characters(url):\n",
    "    return not re.match(\"^((?!-)[A-Za-z0–9-]{1,}(?<!-)\\.)+[A-Za-z0-9]{1,}$\", url)\n",
    "\n",
    "# e.g contains_special_characters(wikipedia.org) \n",
    "# expected output False\n",
    "\n",
    "# e.g contains_special_characters(wikipedi@.org) \n",
    "# expected output True\n",
    "\n",
    "\n",
    "\n",
    "# Helper function to find out if the first TLD is more common than the second TLD\n",
    "\n",
    "# Input - 2 TLDs\n",
    "# Output - True / False\n",
    "\n",
    "def tld_more_common(tld1, tld2):    \n",
    "    sus_index = 10\n",
    "    legit_index = 10\n",
    "    if sus_tld in tlds:\n",
    "        sus_index = tlds.index(sus_tld)\n",
    "    if legit_tld in tlds:\n",
    "        legit_index = tlds.index(legit_tld)\n",
    "    if legit_index > sus_index:\n",
    "        return True\n",
    "    elif legit_index < sus_index:\n",
    "        return False\n",
    "\n",
    "# e.g exact_tld_swap(com, org) \n",
    "# expected output True\n",
    "\n",
    "# e.g exact_tld_swap(org, com) \n",
    "# expected output False\n",
    "\n",
    "\n",
    "\n",
    "# Helper function to find out \n",
    "\n",
    "# Input - 2 URLs\n",
    "# Output - True / False\n",
    "\n",
    "def edit_distance(url1, url2):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_typo(sus_url, legit_url):\n",
    "    # preparing \n",
    "    result = {\"suspicious url\": sus_url, \"original url\": legit_url, \"result\": 0, \"reasons_typosquat\": [], \"reasons_typo\": []}\n",
    "    \n",
    "    # extracting SLD from both URLs\n",
    "    sus_sld = extract_sld(sus_url)\n",
    "    legit_sld = extract_sld(legit_url)\n",
    "    \n",
    "    # extracting TLD from both URLs\n",
    "    sus_tld = extract_tld(sus_url)\n",
    "    legit_tld = extract_tld(legit_url)\n",
    "    \n",
    "    # retrieving length of both URLs\n",
    "    sus_len = len(sus_url)\n",
    "    legit_len = len(legit_url)\n",
    "    \n",
    "    # check for illegal special characters\n",
    "    if contains_special_character(sus_url):\n",
    "        result[\"result\"] = 1\n",
    "        result[\"reasons_typosquat\"].append(\"Illegal characters found in url\")\n",
    "        return result\n",
    "    \n",
    "    # check for exact TLD swap\n",
    "    if sus_sld == legit_sld and sus_tld != legit_tld:\n",
    "        if tld_more_common(sus_tld, legit_tld):\n",
    "            result[\"reasons_typo\"].append(\"TLD is more common\")\n",
    "        else:\n",
    "            result[\"result\"] = 1\n",
    "            result[\"reasons_typosquat\"].append(\"TLD is less common\")\n",
    "            return result\n",
    "    \n",
    "    # check edit distance\n",
    "    ld = ls.levenshtein(sus_url, legit_url)\n",
    "    \n",
    "    # if only 1 edit\n",
    "    if ld == 1:\n",
    "        result[\"result\"] = 1\n",
    "        result[\"reasons_typosquat\"].append(\"Edit distance is only 1\")\n",
    "        return result\n",
    "    \n",
    "    # if exactly 2 edits\n",
    "    elif ld == 2:\n",
    "        \n",
    "        # if lengths are equal, two characters were swapped\n",
    "        if sus_len == legit_len: \n",
    "            for i in range(len(legit_url)):\n",
    "                if sus_url[i] != legit_url[i]:\n",
    "                    if euclidean_distance(sus_url[i], legit_url[i]) > 1.5:\n",
    "                        result[\"result\"] = 1\n",
    "                        result[\"reasons_typosquat\"].append(\"'{}' key and '{}' key are too far apart\".format(sus_url[i], legit_url[i]))\n",
    "                    else:\n",
    "                            result[\"reasons_typo\"].append(\"Wrong character is within ED boundary\")\n",
    "        \n",
    "        # if sus_len == legit_len - 1, missing 1 char, swapped 1\n",
    "        elif sus_len == legit_len - 1:\n",
    "            pass\n",
    "            # check swapped character\n",
    "        \n",
    "        # if sus_len == legit_len + 1, extra 1 char, swapped 1\n",
    "        elif sus_len == legit_len + 1:\n",
    "            pass\n",
    "            # check swapped char\n",
    "            # check extra char\n",
    "            \n",
    "        # if sus_len == legit_len -2, missing 2 characters, TYPO\n",
    "        elif sus_len == legit_len - 2:\n",
    "            result[\"reasons_typo\"].append(\"Only 2 characters are missing\")\n",
    "    \n",
    "    # if 3 or more edits\n",
    "    else:\n",
    "        result[\"result\"] = \"Inconclusive\"\n",
    "        return result\n",
    "    \n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
