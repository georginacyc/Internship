{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements \n",
    "<ul>\n",
    "    <li>Your function should account for keyboard distance for all characters (even special characters)</li>\n",
    "    <li>Your function should account for levenshtein distance as well. If there are more typos, then the score should lean more towards 1 (since it is unlikely for a user to make so many typos)</li>\n",
    "    <li>Think about if it is more likely to make a horizontal typo vs a vertical typo, you may want to assign a weight to differentiate the typos</li>\n",
    "    <li>Think about the case when the strings have different lengths and how you should handle it</li>\n",
    "    <li>Think about if it is necessary to distinguish if the character is already very far away (e.g wikip9dia.org vs wikip0dia.org), both are most likely typosquats, is there a need for a different score? How many keyboard characters away then should I consider it to be not a typo vs not typo?</li>\n",
    "    <li>Try to think of any other conditions / requirements that I may have missed out, and feel free to suggest any</li>\n",
    "</ul>\n",
    "\n",
    "what about swapped letters, one-too-many letters\n",
    "\n",
    "numbers above qwertyuiop are possible typos, but some may be intended typosquats (i.e. o -> 0; E -> 3 ?)\n",
    "\n",
    "what about when a user presses 2 keys on accident? e.g. wikoipedia -> presses \"o\" and \"k\" when trying to press \"k\"\n",
    "\n",
    "are special chars/homoglyphs legal in the url box?\n",
    "\n",
    "what if they miss a letter?\n",
    "\n",
    "\n",
    "[python-Levenshtein PyPI](https://pypi.org/project/python-Levenshtein/)\n",
    "\n",
    "[euclidean distance using numpy (stack overflow)](https://stackoverflow.com/questions/1401712/how-can-the-euclidean-distance-be-calculated-with-numpy) (may help make calculating ED more efficient?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import *\n",
    "import dnstwist\n",
    "from tldextract import extract\n",
    "import pylev as ls\n",
    "import numpy as np\n",
    "import difflib as dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "2.8284271247461903\n"
     ]
    }
   ],
   "source": [
    "keyboard_cartesian = {\n",
    "                        \"1\": {\"y\": -1, \"x\": 0},\n",
    "                        \"2\": {\"y\": -1, \"x\": 1},\n",
    "                        \"3\": {\"y\": -1, \"x\": 2},\n",
    "                        \"4\": {\"y\": -1, \"x\": 3},\n",
    "                        \"5\": {\"y\": -1, \"x\": 4},\n",
    "                        \"6\": {\"y\": -1, \"x\": 5},\n",
    "                        \"7\": {\"y\": -1, \"x\": 6},\n",
    "                        \"8\": {\"y\": -1, \"x\": 7},\n",
    "                        \"9\": {\"y\": -1, \"x\": 8},\n",
    "                        \"0\": {\"y\": -1, \"x\": 9},\n",
    "                        \"-\": {\"y\": -1, \"x\": 10},\n",
    "                        \"q\": {\"y\": 0, \"x\": 0},\n",
    "                        \"w\": {\"y\": 0, \"x\": 1},\n",
    "                        \"e\": {\"y\": 0, \"x\": 2},\n",
    "                        \"r\": {\"y\": 0, \"x\": 3},\n",
    "                        \"t\": {\"y\": 0, \"x\": 4},\n",
    "                        \"y\": {\"y\": 0, \"x\": 5},\n",
    "                        \"u\": {\"y\": 0, \"x\": 6},\n",
    "                        \"i\": {\"y\": 0, \"x\": 7},\n",
    "                        \"o\": {\"y\": 0, \"x\": 8},\n",
    "                        \"p\": {\"y\": 0, \"x\": 9},\n",
    "                        \"a\": {\"y\": 1, \"x\": 0},\n",
    "                        \"s\": {\"y\": 1, \"x\": 1},\n",
    "                        \"d\": {\"y\": 1, \"x\": 2},\n",
    "                        \"f\": {\"y\": 1, \"x\": 3},\n",
    "                        \"g\": {\"y\": 1, \"x\": 4},\n",
    "                        \"h\": {\"y\": 1, \"x\": 5},\n",
    "                        \"j\": {\"y\": 1, \"x\": 6},\n",
    "                        \"k\": {\"y\": 1, \"x\": 7},\n",
    "                        \"l\": {\"y\": 1, \"x\": 8},\n",
    "                        \";\": {\"y\": 2, \"x\": 9},\n",
    "                        \"'\": {\"y\": 2, \"x\": 10},\n",
    "                        \"z\": {\"y\": 2, \"x\": 0},\n",
    "                        \"x\": {\"y\": 2, \"x\": 1},\n",
    "                        \"c\": {\"y\": 2, \"x\": 2},\n",
    "                        \"v\": {\"y\": 2, \"x\": 3},\n",
    "                        \"b\": {\"y\": 2, \"x\": 4},\n",
    "                        \"n\": {\"y\": 2, \"x\": 5},\n",
    "                        \"m\": {\"y\": 2, \"x\": 6},\n",
    "                        \",\": {\"y\": 2, \"x\": 7},\n",
    "                        \".\": {\"y\": 2, \"x\": 8},\n",
    "                        \"/\": {\"y\": 2, \"x\": 9}                   \n",
    "                     }\n",
    "\n",
    "def euclidean_distance(a,b):\n",
    "    X = (keyboard_cartesian[a]['x']-keyboard_cartesian[b]['x'])**2\n",
    "    Y = (keyboard_cartesian[a]['y']-keyboard_cartesian[b]['y'])**2\n",
    "    return sqrt(X+Y)\n",
    "\n",
    "print(euclidean_distance('q', 'r'))\n",
    "print(euclidean_distance('q', 'c'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions & Codes For Testing\n",
    "functions to extract TLD & SLD\n",
    "\n",
    "codes to generate various suspicious URLs to test based off of legit URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_domain_and_tld(url):\n",
    "    tsd, td, tsu = extract(url)\n",
    "    return td + '.' + tsu\n",
    "\n",
    "def extract_tld(url):\n",
    "    tsd, td, tsu = extract(url)\n",
    "    return tsu    \n",
    "\n",
    "def extract_sld(url):\n",
    "    tsd, td, tsu = extract(url)\n",
    "    return td\n",
    "\n",
    "# whitelist = ['https://www.bankofsingapore.com/','http://www.ocbc.com','http://www.dbs.com','http://www.uobgroup.com','http://www.bnpparibas.com.sg','http://www.icbc.com.cn/new-branch/xjp/index.htm','https://www.citibank.com.sg/','https://www.maybank2u.com.sg/','http://www.sbising.com','https://www.sc.com/sg/','http://www.icicibank.com','https://www.hsbc.com.sg/','http://www.ccb.com/','https://www.bankofchina.com/sg/','https://www.boi.com.sg/','http://www.jpmorgan.com','http://iob.com','http://www.indian-bank.com','https://www.hlbank.com.sg','http://www.ca-cib.com','http://www.cimb.com/','http://www.bankofamerica.com/','http://www.bbl.co.th/','http://www.bgcpartners.com/','http://www.ebsgroup.si/','http://www.hlf.com.sg/','http://www.sif.com.sg/','http://www.singapurafinance.com.sg/','https://www.mas.gov.sg/']\n",
    "# print(len(whitelist))\n",
    "\n",
    "# print('Number of whitelisted domains input: ', len(whitelist))\n",
    "\n",
    "# whitelist_domains = []\n",
    "# whitelist_slds = []\n",
    "# for url in whitelist:\n",
    "#     whitelist_domains.append(extract_domain_and_tld(url))\n",
    "#     whitelist_slds.append(extract_sld(url))\n",
    "# typosquat = []\n",
    "# for url in whitelist_domains:\n",
    "#     fuzz = dnstwist.DomainFuzz(url)\n",
    "#     fuzz.generate()\n",
    "#     typosquat.extend([x['domain-name'] for x in fuzz.domains])\n",
    "    \n",
    "# print(typosquat[200:260])\n",
    "# #Lookups in sets are much more efficient\n",
    "# typosquat = set(typosquat)\n",
    "\n",
    "# #Delete the original whitelisted domains from the blacklist set\n",
    "# typosquat.difference_update(whitelist_domains)\n",
    "\n",
    "# print('Number of typosquatted urls generated: ', len(typosquat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bankofsingapore.com', 'ocbc.com', 'dbs.com', 'uobgroup.com', 'bnpparibas.com.sg', 'icbc.com.cn', 'citibank.com.sg', 'maybank2u.com.sg', 'sbising.com', 'sc.com']\n",
      "['bankofsingapore', 'ocbc', 'dbs', 'uobgroup', 'bnpparibas', 'icbc', 'citibank', 'maybank2u', 'sbising', 'sc']\n"
     ]
    }
   ],
   "source": [
    "print(whitelist_domains[:10])\n",
    "print(whitelist_slds[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Try\n",
    "simply checking against Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "legit = \"wikipedia.org\"\n",
    "typo = \"wiiipedia.org\" \n",
    "typosqt = \"wikiped1a.org\"\n",
    "\n",
    "def typo_check(url):\n",
    "    for i in range(len(legit)):\n",
    "        if legit[i] != url[i]:\n",
    "            result = euclidean_distance(legit[i], url[i])\n",
    "            if result >= 1.5:\n",
    "                # typosquat\n",
    "                return 1\n",
    "            else:\n",
    "                # typo\n",
    "                return 0\n",
    "\n",
    "print(typo_check(typosqt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T : URL being tested <br>\n",
    "L : Legit URL <br>\n",
    "LD : levenshtein distance <br>\n",
    "ED : euclidean distance <br>\n",
    "\n",
    "Assuming: <br>\n",
    "-> given T, we know what L is. <br>\n",
    "-> no reason to press shift; urls are not case sensitive, and the only legal special character is hyphen, which does not require shift <br>\n",
    "-> if len(T) = len(L) + 1, assume that extra letters are at the end of domain names\n",
    "\n",
    "\n",
    "Then, checks:\n",
    "- if there are special characters in T, typosquat (in domain names: hyphens are allowed, underscores are not)\n",
    "- if length of T = length of L + 1, check how far away the extra letter is from the previous letter. was it fat-fingered?\n",
    "- if length of T = length of L, check LD. if > 2, definitely typosquat\n",
    "- if length of T = length of L AND LD <= 2, check ED. if any error has ED > 1.5, typosquat\n",
    "\n",
    "definitely typosquat:\n",
    "- special char present anywhere\n",
    "- len(T) > len(L) + 1\n",
    "- LD > 2\n",
    "- ED > 1.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refined codes to aid in testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of typosquatted urls generated:  13515\n"
     ]
    }
   ],
   "source": [
    "# does not include: , . / ; '\n",
    "special_characters = ['~', ':', '+', '[', '\\\\', '@', '^', '{', '%', '(', '\"', '*', '|', ',', '&', '<', '`', '}', '_', '=', ']', '!', '>', '?', '#', '$', ')']\n",
    "\n",
    "whitelist = ['https://www.bankofsingapore.com/']\n",
    "\n",
    "whitelist_domains = []\n",
    "whitelist_slds = []\n",
    "for url in whitelist:\n",
    "    whitelist_domains.append(extract_domain_and_tld(url))\n",
    "    whitelist_slds.append(extract_sld(url))\n",
    "typosquat = []\n",
    "for url in whitelist_domains:\n",
    "    fuzz = dnstwist.DomainFuzz(url)\n",
    "    fuzz.generate()\n",
    "    typosquat.extend([x['domain-name'] for x in fuzz.domains])\n",
    "    \n",
    "#Lookups in sets are much more efficient\n",
    "typosquat = set(typosquat)\n",
    "\n",
    "#Delete the original whitelisted domains from the blacklist set\n",
    "typosquat.difference_update(whitelist_domains)\n",
    "\n",
    "print('Number of typosquatted urls generated: ', len(typosquat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## is_typo(url) Function\n",
    "Code Flow:\n",
    "- checks if url has any special characters\n",
    "- checks LD of url against legit URL\n",
    "- checks length of url against lenght of legit URL\n",
    "- checks the ED of any wrong char in url against corresponding char in legit URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return dictionary\n",
    "def is_typo(url):\n",
    "    l = whitelist_domains[0]\n",
    "    \n",
    "    url_sld = extract_sld(url)\n",
    "    l_sld = extract_sld(l)\n",
    "    \n",
    "    url_len = len(url)\n",
    "    l_len = len(l)\n",
    "    \n",
    "    # checks if illegal special characters are present\n",
    "    # TODO regex\n",
    "    if any(char in url for char in special_characters) or \"-\" in extract_tld(url) or url_sld.startswith(\"-\") or url_sld.endswith(\"-\"):\n",
    "        return 1\n",
    "    \n",
    "    # checks LD\n",
    "    if ls.levenshtein(url, l) > 1:\n",
    "        return 1\n",
    "     \n",
    "    # compares lengths\n",
    "    if url_len > l_len + 1:\n",
    "        return 1\n",
    "    \n",
    "    elif url_len < l_len:\n",
    "        # TODO: add more checks here\n",
    "        # missing 1 key = potential typo\n",
    "        return 1\n",
    "    \n",
    "    elif url_len == l_len + 1:\n",
    "        for i in range(len(l)):\n",
    "            if url[i] != l[i]:\n",
    "                url_left = url[i - 1] if i !=0 else None # char on the left of the wrong/extra char\n",
    "                url_middle = url[i] # wrong/extra char\n",
    "                url_right = url[i + 1] # char on the right of wrong/extra char\n",
    "                           \n",
    "#                 print(url_left)\n",
    "#                 print(url_middle)\n",
    "#                 print(url_right)               \n",
    "                    \n",
    "                # potentially redundant check\n",
    "                if l[i] != url_right:\n",
    "                    return 1\n",
    "                \n",
    "                if  euclidean_distance(url_left, url_middle) > 1.5 and euclidean_distance(url_right, url_middle) > 1.5:\n",
    "                    return 1\n",
    "                \n",
    "                # prevent the function from running on the rest of the string\n",
    "                break\n",
    "                \n",
    "    elif url_len == l_len:\n",
    "        for i in range(len(l)):\n",
    "            if url[i] != l[i]:\n",
    "                if euclidean_distance(url[i], l[i]) > 1.5:\n",
    "                    return 1\n",
    "    return 0\n",
    "\n",
    "# for url in typosquat:\n",
    "#     # if typo\n",
    "#     if is_typo(url) == 0:\n",
    "#         print(url)\n",
    "\n",
    "is_typo(\"bankofsingapore.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "<u>Potential Results</u>: <br>\n",
    "0: Typo <br>\n",
    "1: Typosquat\n",
    "\n",
    "<u>Test Cases</u>:\n",
    "1. Legit URL (Expected Result: 0)\n",
    "2. Substitute 1 char with SC (Expected Result: 1)\n",
    "3. Substitute 1 char with hyphen (Expected Result: 1)\n",
    "4. Substitute 1 char with wrong char, within ED boundary (Expected Result: 0)\n",
    "5. Substitute 2 char with wrong char, within ED boundary (Expected Result: 0)\n",
    "6. Substitute 3 char with wrong char, within ED boundary(Expected Result: 1)\n",
    "7. Append 1 char, within ED boundary of last char (Expected Result: 0)\n",
    "8. Append 1 char, exceeding ED boundary (Expected Result: 1)\n",
    "9. Append 2 char, within ED boundary(Expected Result: 1)\n",
    "10. Remove 1 char (Expected Result: 1)\n",
    "    \n",
    "<u>Conclusion/Notes</u>:\n",
    "- need to check entire URL (SLD + TLD) because of potential periods in domain name, which can be removed when extracting SLD\n",
    "- include more checks for suspicious URLs that are shorter than original URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL 0 : 0\n",
      "URL 1 : 1\n",
      "URL 2 : 1\n",
      "URL 3 : 0\n",
      "URL 4 : 0\n",
      "URL 5 : 1\n",
      "URL 6 : 0\n",
      "URL 7 : 1\n",
      "URL 8 : 1\n",
      "URL 9 : 1\n"
     ]
    }
   ],
   "source": [
    "test_cases = [\"bankofsingapore.com\", \n",
    "              \"bankofs!ngapore.com\", \n",
    "              \"bankofsing-pore.com\", \n",
    "              \"bankofsingaporr.com\", \n",
    "              \"bankofsingapoer.com\", \n",
    "              \"bankofsingapier.com\", \n",
    "              \"bankofsingaporee.com\", \n",
    "              \"bankofsingaporep.com\", \n",
    "              \"bankofsingaporeee.com\", \n",
    "              \"bankofsingapor.com\"]\n",
    "\n",
    "for i in range(len(test_cases)):\n",
    "    print(\"URL\", i, \":\", is_typo(test_cases[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
