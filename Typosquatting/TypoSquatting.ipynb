{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements \n",
    "<ul>\n",
    "    <li>Your function should account for keyboard distance for all characters (even special characters)</li>\n",
    "    <li>Your function should account for levenshtein distance as well. If there are more typos, then the score should lean more towards 1 (since it is unlikely for a user to make so many typos)</li>\n",
    "    <li>Think about if it is more likely to make a horizontal typo vs a vertical typo, you may want to assign a weight to differentiate the typos</li>\n",
    "    <li>Think about the case when the strings have different lengths and how you should handle it</li>\n",
    "    <li>Think about if it is necessary to distinguish if the character is already very far away (e.g wikip9dia.org vs wikip0dia.org), both are most likely typosquats, is there a need for a different score? How many keyboard characters away then should I consider it to be not a typo vs not typo?</li>\n",
    "    <li>Try to think of any other conditions / requirements that I may have missed out, and feel free to suggest any</li>\n",
    "</ul>\n",
    "\n",
    "what about swapped letters, one-too-many letters\n",
    "\n",
    "numbers above qwertyuiop are possible typos, but some may be intended typosquats (i.e. o -> 0; E -> 3 ?)\n",
    "\n",
    "what about when a user presses 2 keys on accident? e.g. wikoipedia -> presses \"o\" and \"k\" when trying to press \"k\"\n",
    "\n",
    "are special chars/homoglyphs legal in the url box?\n",
    "\n",
    "what if they miss a letter?\n",
    "\n",
    "\n",
    "[python-Levenshtein PyPI](https://pypi.org/project/python-Levenshtein/)\n",
    "\n",
    "[euclidean distance using numpy (stack overflow)](https://stackoverflow.com/questions/1401712/how-can-the-euclidean-distance-be-calculated-with-numpy) (may help make calculating ED more efficient?)\n",
    "\n",
    "[top 10 most common TLDs](https://www.statista.com/statistics/265677/number-of-internet-top-level-domains-worldwide/)\n",
    "\n",
    "[domain name regex](https://medium.com/@vaghasiyaharryk/how-to-validate-a-domain-name-using-regular-expression-9ab484a1b430)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import *\n",
    "import dnstwist\n",
    "from tldextract import extract\n",
    "import pylev as ls\n",
    "# import Levenshtein as ls\n",
    "import numpy as np\n",
    "import difflib as dl\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "2.8284271247461903\n"
     ]
    }
   ],
   "source": [
    "keyboard_cartesian = {\n",
    "                        \"1\": {\"y\": -1, \"x\": 0},\n",
    "                        \"2\": {\"y\": -1, \"x\": 1},\n",
    "                        \"3\": {\"y\": -1, \"x\": 2},\n",
    "                        \"4\": {\"y\": -1, \"x\": 3},\n",
    "                        \"5\": {\"y\": -1, \"x\": 4},\n",
    "                        \"6\": {\"y\": -1, \"x\": 5},\n",
    "                        \"7\": {\"y\": -1, \"x\": 6},\n",
    "                        \"8\": {\"y\": -1, \"x\": 7},\n",
    "                        \"9\": {\"y\": -1, \"x\": 8},\n",
    "                        \"0\": {\"y\": -1, \"x\": 9},\n",
    "                        \"-\": {\"y\": -1, \"x\": 10},\n",
    "                        \"q\": {\"y\": 0, \"x\": 0},\n",
    "                        \"w\": {\"y\": 0, \"x\": 1},\n",
    "                        \"e\": {\"y\": 0, \"x\": 2},\n",
    "                        \"r\": {\"y\": 0, \"x\": 3},\n",
    "                        \"t\": {\"y\": 0, \"x\": 4},\n",
    "                        \"y\": {\"y\": 0, \"x\": 5},\n",
    "                        \"u\": {\"y\": 0, \"x\": 6},\n",
    "                        \"i\": {\"y\": 0, \"x\": 7},\n",
    "                        \"o\": {\"y\": 0, \"x\": 8},\n",
    "                        \"p\": {\"y\": 0, \"x\": 9},\n",
    "                        \"a\": {\"y\": 1, \"x\": 0},\n",
    "                        \"s\": {\"y\": 1, \"x\": 1},\n",
    "                        \"d\": {\"y\": 1, \"x\": 2},\n",
    "                        \"f\": {\"y\": 1, \"x\": 3},\n",
    "                        \"g\": {\"y\": 1, \"x\": 4},\n",
    "                        \"h\": {\"y\": 1, \"x\": 5},\n",
    "                        \"j\": {\"y\": 1, \"x\": 6},\n",
    "                        \"k\": {\"y\": 1, \"x\": 7},\n",
    "                        \"l\": {\"y\": 1, \"x\": 8},\n",
    "                        \";\": {\"y\": 2, \"x\": 9},\n",
    "                        \"'\": {\"y\": 2, \"x\": 10},\n",
    "                        \"z\": {\"y\": 2, \"x\": 0},\n",
    "                        \"x\": {\"y\": 2, \"x\": 1},\n",
    "                        \"c\": {\"y\": 2, \"x\": 2},\n",
    "                        \"v\": {\"y\": 2, \"x\": 3},\n",
    "                        \"b\": {\"y\": 2, \"x\": 4},\n",
    "                        \"n\": {\"y\": 2, \"x\": 5},\n",
    "                        \"m\": {\"y\": 2, \"x\": 6},\n",
    "                        \",\": {\"y\": 2, \"x\": 7},\n",
    "                        \".\": {\"y\": 2, \"x\": 8},\n",
    "                        \"/\": {\"y\": 2, \"x\": 9}                   \n",
    "                     }\n",
    "\n",
    "def euclidean_distance(a,b):\n",
    "    X = (keyboard_cartesian[a]['x']-keyboard_cartesian[b]['x'])**2\n",
    "    Y = (keyboard_cartesian[a]['y']-keyboard_cartesian[b]['y'])**2\n",
    "    return sqrt(X+Y)\n",
    "\n",
    "print(euclidean_distance('q', 'r'))\n",
    "print(euclidean_distance('q', 'c'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wikipedia.org']\n",
      "['wikipedia']\n"
     ]
    }
   ],
   "source": [
    "print(whitelist_domains[:10])\n",
    "print(whitelist_slds[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Try\n",
    "simply checking against Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "legit = \"wikipedia.org\"\n",
    "typo = \"wiiipedia.org\" \n",
    "typosqt = \"wikiped1a.org\"\n",
    "\n",
    "def typo_check(url):\n",
    "    for i in range(len(legit)):\n",
    "        if legit[i] != url[i]:\n",
    "            result = euclidean_distance(legit[i], url[i])\n",
    "            if result >= 1.5:\n",
    "                # typosquat\n",
    "                return 1\n",
    "            else:\n",
    "                # typo\n",
    "                return 0\n",
    "\n",
    "print(typo_check(typosqt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T : URL being tested <br>\n",
    "L : Legit URL <br>\n",
    "LD : levenshtein distance <br>\n",
    "ED : euclidean distance <br>\n",
    "\n",
    "Assumptions: <br>\n",
    "<ul >\n",
    "    <li>Given the suspicious URL (S), its genuine URL (G) is known</li>\n",
    "    <li>S has an edit distance of at least 1</li>\n",
    "    <li>There is no reason to press shift; urls are not case sensitive, and the only legal special character is hyphen, which does not require shift</li>\n",
    "    <li>Hyphens are only allowed in between characters in domain names (i.e. domain names cannot start/end with hyphens, and the top level domain cannot contain hyphen)</li>\n",
    "    <li>T is assumed to be a typosquat when it meets one of the following:\n",
    "        <ol>\n",
    "            <li>Contains illegal special characters</li>\n",
    "            <li>Has a Levenshtein distance of more than 1</li>\n",
    "            <li>Is shorter than len(G) - 1</li>\n",
    "            <li>Is longer than len(G) + 1</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "Then, checks:\n",
    "- if there are special characters in T, typosquat (in domain names: hyphens are allowed, underscores are not)\n",
    "- if length of T = length of L + 1, check how far away the extra letter is from the previous letter. was it fat-fingered?\n",
    "- if length of T = length of L, check LD. if > 2, definitely typosquat\n",
    "- if length of T = length of L AND LD <= 2, check ED. if any error has ED > 1.5, typosquat\n",
    "\n",
    "definitely typosquat:\n",
    "- special char present anywhere\n",
    "- len(T) > len(L) + 1\n",
    "- LD > 2\n",
    "- ED > 1.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions & Codes For Testing\n",
    "functions to extract TLD & SLD\n",
    "\n",
    "codes to generate various suspicious URLs to test based off of legit URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of typosquatted urls generated:  7900\n"
     ]
    }
   ],
   "source": [
    "def extract_domain_and_tld(url):\n",
    "    tsd, td, tsu = extract(url)\n",
    "    return td + '.' + tsu\n",
    "\n",
    "def extract_tld(url):\n",
    "    tsd, td, tsu = extract(url)\n",
    "    return tsu    \n",
    "\n",
    "def extract_sld(url):\n",
    "    tsd, td, tsu = extract(url)\n",
    "    return td\n",
    "\n",
    "# does not include: , . / ; '\n",
    "special_characters = ['~', ':', '+', '[', '\\\\', '@', '^', '{', '%', '(', '\"', '*', '|', ',', '&', '<', '`', '}', '_', '=', ']', '!', '>', '?', '#', '$', ')']\n",
    "\n",
    "# used tuple because theres no need for this container to be mutable and tuples are faster\n",
    "tlds = (\"com\", \"ru\", \"org\", \"net\", \"in\", \"ir\", \"au\", \"uk\", \"de\", \"br\")\n",
    "\n",
    "# whitelist = ['https://www.bankofsingapore.com/']\n",
    "whitelist = ['https://www.wikipedia.org/']\n",
    "\n",
    "\n",
    "whitelist_domains = []\n",
    "whitelist_slds = []\n",
    "for url in whitelist:\n",
    "    whitelist_domains.append(extract_domain_and_tld(url))\n",
    "    whitelist_slds.append(extract_sld(url))\n",
    "typosquat = []\n",
    "for url in whitelist_domains:\n",
    "    fuzz = dnstwist.DomainFuzz(url)\n",
    "    fuzz.generate()\n",
    "    typosquat.extend([x['domain-name'] for x in fuzz.domains])\n",
    "    \n",
    "#Lookups in sets are much more efficient\n",
    "typosquat = set(typosquat)\n",
    "\n",
    "#Delete the original whitelisted domains from the blacklist set\n",
    "typosquat.difference_update(whitelist_domains)\n",
    "\n",
    "print('Number of typosquatted urls generated: ', len(typosquat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## is_typo(url) Function\n",
    "Code Flow:\n",
    "- checks if url has any special characters\n",
    "- checks LD of url against legit URL\n",
    "- checks the TLD of url agains legit URL\n",
    "- checks length of url against lenght of legit URL\n",
    "- checks the ED of any wrong char in url against corresponding char in legit URL\n",
    "\n",
    "todo:\n",
    "- also check tld\n",
    "- add extra chars to any of the letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'suspicious url': 'wikipedia.org', 'original url': 'wikipedia.org', 'result': 0, 'reasons_typosquat': [], 'reasons_typo': []}\n",
      "{'suspicious url': 'wikipedip.org', 'original url': 'wikipedia.org', 'result': 1, 'reasons_typosquat': [\"'p' key and 'a' key are too far apart\"], 'reasons_typo': ['Edit distance is only one']}\n",
      "{'suspicious url': 'wikipedia.com', 'original url': 'wikipedia.org', 'result': 1, 'reasons_typosquat': ['Edit distance more than 1', \"'c' key and 'o' key are too far apart\", \"'o' key and 'r' key are too far apart\", \"'m' key and 'g' key are too far apart\"], 'reasons_typo': ['TLD is more common']}\n"
     ]
    }
   ],
   "source": [
    "def is_typo(url):\n",
    "    l = whitelist_domains[0]\n",
    "    \n",
    "    result = {\"suspicious url\": url, \"original url\": l, \"result\": 0, \"reasons_typosquat\": [], \"reasons_typo\": []}\n",
    "    \n",
    "    url_sld = extract_sld(url)\n",
    "    l_sld = extract_sld(l)\n",
    "    \n",
    "    url_tld = extract_tld(url)\n",
    "    l_tld = extract_tld(l)\n",
    "    \n",
    "    url_len = len(url)\n",
    "    l_len = len(l)\n",
    "    \n",
    "    # checks if illegal special characters are present\n",
    "    if not re.match(\"^[^-][a-zA-Z0-9-]{1,}[^-][.]{1}[a-zA-Z0-9]{1,}$\", url):\n",
    "        result[\"result\"] = 1\n",
    "        result[\"reasons_typosquat\"].append(\"Illegal characters found in url\")\n",
    "    \n",
    "    # checks LD\n",
    "    if ls.levenshtein(url, l) > 1:\n",
    "        result[\"result\"] = 1\n",
    "        result[\"reasons_typosquat\"].append(\"Edit distance more than 1\")\n",
    "    elif ls.levenshtein(url, l) == 1:\n",
    "        result[\"reasons_typo\"].append(\"Edit distance is only one\")\n",
    "    \n",
    "    # checks TLD\n",
    "    if url_tld != l_tld:\n",
    "        url_index = 10\n",
    "        l_index = 10\n",
    "        if url_tld in tlds:\n",
    "            url_index = tlds.index(url_tld)\n",
    "        if l_tld in tlds:\n",
    "            l_index = tlds.index(l_tld)\n",
    "        if l_index > url_index:\n",
    "            result[\"reasons_typo\"].append(\"TLD is more common\")\n",
    "            \n",
    "     \n",
    "    # compares lengths\n",
    "    if url_len > l_len + 1:\n",
    "        result[\"result\"] = 1\n",
    "        result[\"reasons_typosquat\"].append(\"Too long\")\n",
    "    \n",
    "    elif url_len < l_len - 1:\n",
    "        result[\"result\"] = 1\n",
    "        result[\"reasons_typosquat\"].append(\"Too short\")\n",
    "    \n",
    "    elif url_len == l_len + 1:\n",
    "        for i in range(len(l)):\n",
    "            if url[i] != l[i]:\n",
    "                url_left = url[i - 1] if i != 0 else None # char on the left of the wrong/extra char\n",
    "                url_middle = url[i] # wrong/extra char\n",
    "                url_right = url[i + 1] if i + 1 < len(url) else None # char on the right of wrong/extra char          \n",
    "                \n",
    "                if url_left == None:\n",
    "                    if euclidean_distance(url_right, url_middle) > 1.5:\n",
    "                        result[\"result\"] = 1\n",
    "                        result[\"reasons_typosquat\"].append(\"Extra character too far from characters next to it\")\n",
    "                    else:\n",
    "                        result[\"reasons_typo\"].append(\"Extra character is within ED boundary\")\n",
    "                        \n",
    "                elif url_right == None:\n",
    "                    if euclidean_distance(url_left, url_middle) > 1.5:\n",
    "                        result[\"result\"] = 1\n",
    "                        result[\"reasons_typosquat\"].append(\"Extra character too far from characters next to it\")\n",
    "                    else:\n",
    "                        result[\"reasons_typo\"].append(\"Extra character is within ED boundary\")\n",
    "                else:\n",
    "                    if euclidean_distance(url_left, url_middle) > 1.5 and euclidean_distance(url_right, url_middle) > 1.5:\n",
    "                        result[\"result\"] = 1\n",
    "                        result[\"reasons_typosquat\"].append(\"Extra character too far from characters next to it\")\n",
    "                    else:\n",
    "                        result[\"reasons_typo\"].append(\"Extra character is within ED boundary\")\n",
    "                \n",
    "                # prevent the function from running on the rest of the string\n",
    "                break\n",
    "                \n",
    "    elif url_len == l_len:\n",
    "        for i in range(len(l)):\n",
    "            if url[i] != l[i]:\n",
    "                if euclidean_distance(url[i], l[i]) > 1.5:\n",
    "                    result[\"result\"] = 1\n",
    "                    result[\"reasons_typosquat\"].append(\"'{}' key and '{}' key are too far apart\".format(url[i], l[i]))\n",
    "                else:\n",
    "                        result[\"reasons_typo\"].append(\"Wrong character is within ED boundary\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(is_typo(\"wikipedia.org\"))\n",
    "print(is_typo(\"wikipedip.org\")) # equal parts typo and typosquat, according to my code logic\n",
    "print(is_typo(\"wikipedia.com\")) \n",
    "\n",
    "# for url in list(typosquat)[1010:1020]:\n",
    "#     # if typo\n",
    "#     if is_typo(url)[\"result\"] == 1:\n",
    "#         print(is_typo(url))\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototype 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'suspicious url': 'wikipedia.org', 'original url': 'wikipedia.org', 'result': 0, 'reasons_typosquat': [], 'reasons_typo': []}\n",
      "{'suspicious url': 'wikipediaaaaa.org', 'original url': 'wikipedia.org', 'result': 1, 'reasons_typosquat': ['Too long'], 'reasons_typo': []}\n",
      "{'suspicious url': 'wikipe.org', 'original url': 'wikipedia.org', 'result': 1, 'reasons_typosquat': ['Too short'], 'reasons_typo': []}\n",
      "{'suspicious url': 'wikipedi@.org', 'original url': 'wikipedia.org', 'result': 1, 'reasons_typosquat': ['Illegal characters found in url'], 'reasons_typo': []}\n",
      "{'suspicious url': 'wikipedia.com', 'original url': 'wikipedia.org', 'result': 0, 'reasons_typosquat': [], 'reasons_typo': ['TLD is more common']}\n",
      "{'suspicious url': 'wikipedis.com', 'original url': 'wikipedia.org', 'result': 1, 'reasons_typosquat': ['Too many typos'], 'reasons_typo': ['TLD is more common', 'Wrong character is within ED boundary']}\n",
      "{'suspicious url': 'wikipedai.org', 'original url': 'wikipedia.org', 'result': 1, 'reasons_typosquat': ['Edit distance more than 1'], 'reasons_typo': []}\n",
      "{'suspicious url': 'wikipedih.org', 'original url': 'wikipedia.org', 'result': 1, 'reasons_typosquat': [\"'h' key and 'a' key are too far apart\"], 'reasons_typo': []}\n",
      "{'suspicious url': 'wikipediap.org', 'original url': 'wikipedia.org', 'result': 1, 'reasons_typosquat': ['Extra character too far from characters next to it'], 'reasons_typo': []}\n"
     ]
    }
   ],
   "source": [
    "def is_typo(url):\n",
    "    l = whitelist_domains[0]\n",
    "    \n",
    "    result = {\"suspicious url\": url, \"original url\": l, \"result\": 0, \"reasons_typosquat\": [], \"reasons_typo\": []}\n",
    "    \n",
    "    url_sld = extract_sld(url)\n",
    "    l_sld = extract_sld(l)\n",
    "    \n",
    "    url_tld = extract_tld(url)\n",
    "    l_tld = extract_tld(l)\n",
    "    \n",
    "    url_len = len(url)\n",
    "    l_len = len(l)\n",
    "    \n",
    "    # checks length\n",
    "    if l_len + 1 < url_len:\n",
    "        result[\"result\"] = 1\n",
    "        result[\"reasons_typosquat\"].append(\"Too long\")\n",
    "        return result\n",
    "    elif url_len < l_len - 1:\n",
    "        result[\"result\"] = 1\n",
    "        result[\"reasons_typosquat\"].append(\"Too short\")\n",
    "        return result\n",
    "    \n",
    "    # checks if illegal special characters are present\n",
    "    if not re.match(\"^((?!-)[A-Za-z0–9-]{1,}(?<!-)\\.)+[A-Za-z0-9]{1,}$\", url):\n",
    "        result[\"result\"] = 1\n",
    "        result[\"reasons_typosquat\"].append(\"Illegal characters found in url\")\n",
    "        return result\n",
    "    \n",
    "    # checks TLD; WIP\n",
    "    if url_tld != l_tld:\n",
    "        url_index = 10\n",
    "        l_index = 10\n",
    "        if url_tld in tlds:\n",
    "            url_index = tlds.index(url_tld)\n",
    "        if l_tld in tlds:\n",
    "            l_index = tlds.index(l_tld)\n",
    "        if l_index > url_index:\n",
    "            result[\"reasons_typo\"].append(\"TLD is more common\")\n",
    "            \n",
    "            # reassign values so next checks will only be on sld, as tld mistakes have already been ruled out\n",
    "            url = url_sld\n",
    "            l = l_sld\n",
    "            url_len = len(url)\n",
    "            l_len = len(l)\n",
    "    \n",
    "    \n",
    "    # levenshtein distance check\n",
    "    if ls.levenshtein(url, l) > 1:\n",
    "        result[\"result\"] = 1\n",
    "        result[\"reasons_typosquat\"].append(\"Edit distance more than 1\")\n",
    "        return result\n",
    "    else:\n",
    "        # length checks\n",
    "        if url_len == l_len:\n",
    "            for i in range(len(l)):\n",
    "                if url[i] != l[i]:\n",
    "                    if euclidean_distance(url[i], l[i]) > 1.5:\n",
    "                        result[\"result\"] = 1\n",
    "                        result[\"reasons_typosquat\"].append(\"'{}' key and '{}' key are too far apart\".format(url[i], l[i]))\n",
    "                    else:\n",
    "                            result[\"reasons_typo\"].append(\"Wrong character is within ED boundary\")\n",
    "        \n",
    "        elif url_len == l_len + 1:\n",
    "            for i in range(len(l)):\n",
    "                if url[i] != l[i]:\n",
    "                    url_left = url[i - 1] if i != 0 else None # char on the left of the wrong/extra char\n",
    "                    url_middle = url[i] # wrong/extra char\n",
    "                    url_right = url[i + 1] if i + 1 < len(url) else None # char on the right of wrong/extra char          \n",
    "\n",
    "                    if url_left == None:\n",
    "                        if euclidean_distance(url_right, url_middle) > 1.5:\n",
    "                            result[\"result\"] = 1\n",
    "                            result[\"reasons_typosquat\"].append(\"Extra character too far from characters next to it\")\n",
    "                        else:\n",
    "                            result[\"reasons_typo\"].append(\"Extra character is within ED boundary\")\n",
    "\n",
    "                    elif url_right == None:\n",
    "                        if euclidean_distance(url_left, url_middle) > 1.5:\n",
    "                            result[\"result\"] = 1\n",
    "                            result[\"reasons_typosquat\"].append(\"Extra character too far from characters next to it\")\n",
    "                        else:\n",
    "                            result[\"reasons_typo\"].append(\"Extra character is within ED boundary\")\n",
    "                    else:\n",
    "                        if euclidean_distance(url_left, url_middle) > 1.5 and euclidean_distance(url_right, url_middle) > 1.5:\n",
    "                            result[\"result\"] = 1\n",
    "                            result[\"reasons_typosquat\"].append(\"Extra character too far from characters next to it\")\n",
    "                        else:\n",
    "                            result[\"reasons_typo\"].append(\"Extra character is within ED boundary\")\n",
    "\n",
    "                    # prevent the function from running on the rest of the string\n",
    "                    break\n",
    "        \n",
    "        elif url_len == l_len - 1:\n",
    "            result[\"reasons_typo\"].append(\"Only missing 1 character\")\n",
    "            \n",
    "        if len(result[\"reasons_typo\"]) > 1:\n",
    "            result[\"result\"] = 1\n",
    "            result[\"reasons_typosquat\"].append(\"Too many typos\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# original url\n",
    "print(is_typo(\"wikipedia.org\"))\n",
    "\n",
    "# too long\n",
    "print(is_typo(\"wikipediaaaaa.org\"))\n",
    "\n",
    "# too short\n",
    "print(is_typo(\"wikipe.org\"))\n",
    "\n",
    "# with special character\n",
    "print(is_typo(\"wikipedi@.org\"))\n",
    "\n",
    "# exact TLD swap\n",
    "print(is_typo(\"wikipedia.com\"))\n",
    "\n",
    "# 2 typos\n",
    "print(is_typo(\"wikipedis.com\"))\n",
    "\n",
    "# LD = 2\n",
    "print(is_typo(\"wikipedai.org\"))\n",
    "\n",
    "# lengths are equal\n",
    "print(is_typo(\"wikipedih.org\"))\n",
    "\n",
    "# one character too long\n",
    "print(is_typo(\"wikipediap.org\"))\n",
    "\n",
    "# count = 0\n",
    "# for url in list(typosquat):\n",
    "#     result = is_typo2(url)\n",
    "#     if result[\"result\"] == 0:\n",
    "#         count += 1\n",
    "#         print(result)\n",
    "    \n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "<u>Potential Results</u>: <br>\n",
    "0: Typo <br>\n",
    "1: Typosquat\n",
    "\n",
    "<u>Test Cases</u>:\n",
    "1. Legit URL (Expected Result: 0)\n",
    "2. Substitute 1 char with SC (Expected Result: 1)\n",
    "3. Substitute 1 char with hyphen (Expected Result: 1)\n",
    "4. Substitute 1 char with wrong char, within ED boundary (Expected Result: 0)\n",
    "5. Substitute 2 char with wrong char, within ED boundary (Expected Result: 0)\n",
    "6. Substitute 3 char with wrong char, within ED boundary(Expected Result: 1)\n",
    "7. Append 1 char, within ED boundary of last char (Expected Result: 0)\n",
    "8. Append 1 char, exceeding ED boundary (Expected Result: 1)\n",
    "9. Append 2 char, within ED boundary(Expected Result: 1)\n",
    "10. Remove 1 char (Expected Result: 1)\n",
    "    \n",
    "<u>Conclusion/Notes</u>:\n",
    "- need to recheck for special characters, as they are not accounted for in the next checks after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL 0 : {'suspicious url': 'bankofsingapore.com', 'original url': 'bankofsingapore.com', 'result': 0, 'reasons': []}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'!'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-156-35319a8a2127>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_cases\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"URL\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\":\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_typo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_cases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-154-7849f9f6c18d>\u001b[0m in \u001b[0;36mis_typo\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0meuclidean_distance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1.5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"result\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"reasons\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'{}' key and '{}' key are too far apart\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-151-ebe837127f37>\u001b[0m in \u001b[0;36meuclidean_distance\u001b[1;34m(a, b)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0meuclidean_distance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mkeyboard_cartesian\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mkeyboard_cartesian\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mkeyboard_cartesian\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mkeyboard_cartesian\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '!'"
     ]
    }
   ],
   "source": [
    "test_cases = [\"bankofsingapore.com\", \n",
    "              \"bankofs!ngapore.com\", \n",
    "              \"bankofsing-pore.com\", \n",
    "              \"bankofsingaporr.com\", \n",
    "              \"bankofsingapoer.com\", \n",
    "              \"bankofsingapier.com\", \n",
    "              \"bankofsingaporee.com\", \n",
    "              \"bankofsingaporep.com\", \n",
    "              \"bankofsingaporeee.com\", \n",
    "              \"bankofsingapor.com\"]\n",
    "\n",
    "for i in range(len(test_cases)):\n",
    "    print(\"URL\", i, \":\", is_typo(test_cases[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
