{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements \n",
    "<ul>\n",
    "    <li>Your function should account for keyboard distance for all characters (even special characters)</li>\n",
    "    <li>Your function should account for levenshtein distance as well. If there are more typos, then the score should lean more towards 1 (since it is unlikely for a user to make so many typos)</li>\n",
    "    <li>Think about if it is more likely to make a horizontal typo vs a vertical typo, you may want to assign a weight to differentiate the typos</li>\n",
    "    <li>Think about the case when the strings have different lengths and how you should handle it</li>\n",
    "    <li>Think about if it is necessary to distinguish if the character is already very far away (e.g wikip9dia.org vs wikip0dia.org), both are most likely typosquats, is there a need for a different score? How many keyboard characters away then should I consider it to be not a typo vs not typo?</li>\n",
    "    <li>Try to think of any other conditions / requirements that I may have missed out, and feel free to suggest any</li>\n",
    "</ul>\n",
    "\n",
    "what about swapped letters, one-too-many letters\n",
    "\n",
    "numbers above qwertyuiop are possible typos, but some may be intended typosquats (i.e. o -> 0; E -> 3 ?)\n",
    "\n",
    "what about when a user presses 2 keys on accident? e.g. wikoipedia -> presses \"o\" and \"k\" when trying to press \"k\"\n",
    "\n",
    "are special chars/homoglyphs legal in the url box?\n",
    "\n",
    "what if they miss a letter?\n",
    "\n",
    "\n",
    "[python-Levenshtein PyPI](https://pypi.org/project/python-Levenshtein/)\n",
    "\n",
    "[euclidean distance using numpy (stack overflow)](https://stackoverflow.com/questions/1401712/how-can-the-euclidean-distance-be-calculated-with-numpy) (may help make calculating ED more efficient?)\n",
    "\n",
    "[top 10 most common TLDs](https://www.statista.com/statistics/265677/number-of-internet-top-level-domains-worldwide/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import *\n",
    "import dnstwist\n",
    "from tldextract import extract\n",
    "import pylev as ls\n",
    "import numpy as np\n",
    "import difflib as dl\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "2.8284271247461903\n"
     ]
    }
   ],
   "source": [
    "keyboard_cartesian = {\n",
    "                        \"1\": {\"y\": -1, \"x\": 0},\n",
    "                        \"2\": {\"y\": -1, \"x\": 1},\n",
    "                        \"3\": {\"y\": -1, \"x\": 2},\n",
    "                        \"4\": {\"y\": -1, \"x\": 3},\n",
    "                        \"5\": {\"y\": -1, \"x\": 4},\n",
    "                        \"6\": {\"y\": -1, \"x\": 5},\n",
    "                        \"7\": {\"y\": -1, \"x\": 6},\n",
    "                        \"8\": {\"y\": -1, \"x\": 7},\n",
    "                        \"9\": {\"y\": -1, \"x\": 8},\n",
    "                        \"0\": {\"y\": -1, \"x\": 9},\n",
    "                        \"-\": {\"y\": -1, \"x\": 10},\n",
    "                        \"q\": {\"y\": 0, \"x\": 0},\n",
    "                        \"w\": {\"y\": 0, \"x\": 1},\n",
    "                        \"e\": {\"y\": 0, \"x\": 2},\n",
    "                        \"r\": {\"y\": 0, \"x\": 3},\n",
    "                        \"t\": {\"y\": 0, \"x\": 4},\n",
    "                        \"y\": {\"y\": 0, \"x\": 5},\n",
    "                        \"u\": {\"y\": 0, \"x\": 6},\n",
    "                        \"i\": {\"y\": 0, \"x\": 7},\n",
    "                        \"o\": {\"y\": 0, \"x\": 8},\n",
    "                        \"p\": {\"y\": 0, \"x\": 9},\n",
    "                        \"a\": {\"y\": 1, \"x\": 0},\n",
    "                        \"s\": {\"y\": 1, \"x\": 1},\n",
    "                        \"d\": {\"y\": 1, \"x\": 2},\n",
    "                        \"f\": {\"y\": 1, \"x\": 3},\n",
    "                        \"g\": {\"y\": 1, \"x\": 4},\n",
    "                        \"h\": {\"y\": 1, \"x\": 5},\n",
    "                        \"j\": {\"y\": 1, \"x\": 6},\n",
    "                        \"k\": {\"y\": 1, \"x\": 7},\n",
    "                        \"l\": {\"y\": 1, \"x\": 8},\n",
    "                        \";\": {\"y\": 2, \"x\": 9},\n",
    "                        \"'\": {\"y\": 2, \"x\": 10},\n",
    "                        \"z\": {\"y\": 2, \"x\": 0},\n",
    "                        \"x\": {\"y\": 2, \"x\": 1},\n",
    "                        \"c\": {\"y\": 2, \"x\": 2},\n",
    "                        \"v\": {\"y\": 2, \"x\": 3},\n",
    "                        \"b\": {\"y\": 2, \"x\": 4},\n",
    "                        \"n\": {\"y\": 2, \"x\": 5},\n",
    "                        \"m\": {\"y\": 2, \"x\": 6},\n",
    "                        \",\": {\"y\": 2, \"x\": 7},\n",
    "                        \".\": {\"y\": 2, \"x\": 8},\n",
    "                        \"/\": {\"y\": 2, \"x\": 9}                   \n",
    "                     }\n",
    "\n",
    "def euclidean_distance(a,b):\n",
    "    X = (keyboard_cartesian[a]['x']-keyboard_cartesian[b]['x'])**2\n",
    "    Y = (keyboard_cartesian[a]['y']-keyboard_cartesian[b]['y'])**2\n",
    "    return sqrt(X+Y)\n",
    "\n",
    "print(euclidean_distance('q', 'r'))\n",
    "print(euclidean_distance('q', 'c'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions & Codes For Testing\n",
    "functions to extract TLD & SLD\n",
    "\n",
    "codes to generate various suspicious URLs to test based off of legit URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_domain_and_tld(url):\n",
    "    tsd, td, tsu = extract(url)\n",
    "    return td + '.' + tsu\n",
    "\n",
    "def extract_tld(url):\n",
    "    tsd, td, tsu = extract(url)\n",
    "    return tsu    \n",
    "\n",
    "def extract_sld(url):\n",
    "    tsd, td, tsu = extract(url)\n",
    "    return td\n",
    "\n",
    "# whitelist = ['https://www.bankofsingapore.com/','http://www.ocbc.com','http://www.dbs.com','http://www.uobgroup.com','http://www.bnpparibas.com.sg','http://www.icbc.com.cn/new-branch/xjp/index.htm','https://www.citibank.com.sg/','https://www.maybank2u.com.sg/','http://www.sbising.com','https://www.sc.com/sg/','http://www.icicibank.com','https://www.hsbc.com.sg/','http://www.ccb.com/','https://www.bankofchina.com/sg/','https://www.boi.com.sg/','http://www.jpmorgan.com','http://iob.com','http://www.indian-bank.com','https://www.hlbank.com.sg','http://www.ca-cib.com','http://www.cimb.com/','http://www.bankofamerica.com/','http://www.bbl.co.th/','http://www.bgcpartners.com/','http://www.ebsgroup.si/','http://www.hlf.com.sg/','http://www.sif.com.sg/','http://www.singapurafinance.com.sg/','https://www.mas.gov.sg/']\n",
    "# print(len(whitelist))\n",
    "\n",
    "# print('Number of whitelisted domains input: ', len(whitelist))\n",
    "\n",
    "# whitelist_domains = []\n",
    "# whitelist_slds = []\n",
    "# for url in whitelist:\n",
    "#     whitelist_domains.append(extract_domain_and_tld(url))\n",
    "#     whitelist_slds.append(extract_sld(url))\n",
    "# typosquat = []\n",
    "# for url in whitelist_domains:\n",
    "#     fuzz = dnstwist.DomainFuzz(url)\n",
    "#     fuzz.generate()\n",
    "#     typosquat.extend([x['domain-name'] for x in fuzz.domains])\n",
    "    \n",
    "# print(typosquat[200:260])\n",
    "# #Lookups in sets are much more efficient\n",
    "# typosquat = set(typosquat)\n",
    "\n",
    "# #Delete the original whitelisted domains from the blacklist set\n",
    "# typosquat.difference_update(whitelist_domains)\n",
    "\n",
    "# print('Number of typosquatted urls generated: ', len(typosquat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bankofsingapore.com', 'ocbc.com', 'dbs.com', 'uobgroup.com', 'bnpparibas.com.sg', 'icbc.com.cn', 'citibank.com.sg', 'maybank2u.com.sg', 'sbising.com', 'sc.com']\n",
      "['bankofsingapore', 'ocbc', 'dbs', 'uobgroup', 'bnpparibas', 'icbc', 'citibank', 'maybank2u', 'sbising', 'sc']\n"
     ]
    }
   ],
   "source": [
    "print(whitelist_domains[:10])\n",
    "print(whitelist_slds[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Try\n",
    "simply checking against Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "legit = \"wikipedia.org\"\n",
    "typo = \"wiiipedia.org\" \n",
    "typosqt = \"wikiped1a.org\"\n",
    "\n",
    "def typo_check(url):\n",
    "    for i in range(len(legit)):\n",
    "        if legit[i] != url[i]:\n",
    "            result = euclidean_distance(legit[i], url[i])\n",
    "            if result >= 1.5:\n",
    "                # typosquat\n",
    "                return 1\n",
    "            else:\n",
    "                # typo\n",
    "                return 0\n",
    "\n",
    "print(typo_check(typosqt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T : URL being tested <br>\n",
    "L : Legit URL <br>\n",
    "LD : levenshtein distance <br>\n",
    "ED : euclidean distance <br>\n",
    "\n",
    "Assuming: <br>\n",
    "-> given T, we know what L is. <br>\n",
    "-> no reason to press shift; urls are not case sensitive, and the only legal special character is hyphen, which does not require shift <br>\n",
    "-> if len(T) = len(L) + 1, assume that extra letters are at the end of domain names\n",
    "\n",
    "\n",
    "Then, checks:\n",
    "- if there are special characters in T, typosquat (in domain names: hyphens are allowed, underscores are not)\n",
    "- if length of T = length of L + 1, check how far away the extra letter is from the previous letter. was it fat-fingered?\n",
    "- if length of T = length of L, check LD. if > 2, definitely typosquat\n",
    "- if length of T = length of L AND LD <= 2, check ED. if any error has ED > 1.5, typosquat\n",
    "\n",
    "definitely typosquat:\n",
    "- special char present anywhere\n",
    "- len(T) > len(L) + 1\n",
    "- LD > 2\n",
    "- ED > 1.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refined codes to aid in testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of typosquatted urls generated:  13515\n"
     ]
    }
   ],
   "source": [
    "# does not include: , . / ; '\n",
    "special_characters = ['~', ':', '+', '[', '\\\\', '@', '^', '{', '%', '(', '\"', '*', '|', ',', '&', '<', '`', '}', '_', '=', ']', '!', '>', '?', '#', '$', ')']\n",
    "\n",
    "# used tuple because theres no need for this container to be mutable and tuples are faster\n",
    "tlds = (\"com\", \"ru\", \"org\", \"net\", \"in\", \"ir\", \"au\", \"uk\", \"de\", \"br\")\n",
    "\n",
    "whitelist = ['https://www.bankofsingapore.com/']\n",
    "\n",
    "whitelist_domains = []\n",
    "whitelist_slds = []\n",
    "for url in whitelist:\n",
    "    whitelist_domains.append(extract_domain_and_tld(url))\n",
    "    whitelist_slds.append(extract_sld(url))\n",
    "typosquat = []\n",
    "for url in whitelist_domains:\n",
    "    fuzz = dnstwist.DomainFuzz(url)\n",
    "    fuzz.generate()\n",
    "    typosquat.extend([x['domain-name'] for x in fuzz.domains])\n",
    "    \n",
    "#Lookups in sets are much more efficient\n",
    "typosquat = set(typosquat)\n",
    "\n",
    "#Delete the original whitelisted domains from the blacklist set\n",
    "typosquat.difference_update(whitelist_domains)\n",
    "\n",
    "print('Number of typosquatted urls generated: ', len(typosquat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## is_typo(url) Function\n",
    "Code Flow:\n",
    "- checks if url has any special characters\n",
    "- checks LD of url against legit URL\n",
    "- checks length of url against lenght of legit URL\n",
    "- checks the ED of any wrong char in url against corresponding char in legit URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-134-9fc692fd77b7>, line 84)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-134-9fc692fd77b7>\"\u001b[1;36m, line \u001b[1;32m84\u001b[0m\n\u001b[1;33m    print(is_typo(\"bankofsingapore.org))\u001b[0m\n\u001b[1;37m                                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "def is_typo(url):\n",
    "    l = whitelist_domains[0]\n",
    "    \n",
    "    result = {\"suspicious url\": url, \"original url\": l, \"result\": 0, \"reasons\": []}\n",
    "    \n",
    "    url_sld = extract_sld(url)\n",
    "    l_sld = extract_sld(l)\n",
    "    \n",
    "    url_tld = extract_tld(url)\n",
    "    l_tld = extract_tld(l)\n",
    "    \n",
    "    url_len = len(url)\n",
    "    l_len = len(l)\n",
    "    \n",
    "    # checks if illegal special characters are present\n",
    "    if not re.match(\"^[^-][a-zA-Z0-9-]{1,}[^-][.]{1}[a-zA-Z0-9]{1,}$\", url):\n",
    "        result[\"result\"] = 1\n",
    "        result[\"reasons\"].append(\"Illegal characters found in url\")\n",
    "    \n",
    "    # checks LD\n",
    "    if ls.levenshtein(url, l) > 1:\n",
    "        result[\"result\"] = 1\n",
    "        result[\"reasons\"].append(\"Edit distance more than 1\")\n",
    "    \n",
    "    if url_tld != l_tld:\n",
    "        url_index = 10\n",
    "        l_index = 10\n",
    "        if url_tld in tlds:\n",
    "            url_index = tlds.index(url_tld)\n",
    "        if l_tld in tlds:\n",
    "            l_index = tlds.index(l_tld)\n",
    "        if l_index < url_index:\n",
    "            result[\"result\"] = 1\n",
    "            result[\"reasons\"].append(\"TLD is less common\")\n",
    "            \n",
    "     \n",
    "    # compares lengths\n",
    "    if url_len > l_len + 1:\n",
    "        result[\"result\"] = 1\n",
    "        result[\"reasons\"].append(\"Too long\")\n",
    "    \n",
    "    elif url_len < l_len - 1:\n",
    "        result[\"result\"] = 1\n",
    "        result[\"reasons\"].append(\"Too short\")\n",
    "    \n",
    "    elif url_len == l_len + 1:\n",
    "        for i in range(len(l)):\n",
    "            if url[i] != l[i]:\n",
    "                url_left = url[i - 1] if i != 0 else None # char on the left of the wrong/extra char\n",
    "                url_middle = url[i] # wrong/extra char\n",
    "                url_right = url[i + 1] if i + 1 < len(url) else None # char on the right of wrong/extra char          \n",
    "                \n",
    "                if url_left == None:\n",
    "                    if euclidean_distance(url_right, url_middle) > 1.5:\n",
    "                        result[\"result\"] = 1\n",
    "                        result[\"reasons\"].append(\"Extra character too far from characters next to it\")\n",
    "                        \n",
    "                elif url_right == None:\n",
    "                    if euclidean_distance(url_left, url_middle) > 1.5:\n",
    "                        result[\"result\"] = 1\n",
    "                        result[\"reasons\"].append(\"Extra character too far from characters next to it\")\n",
    "                else:\n",
    "                    if euclidean_distance(url_left, url_middle) > 1.5 and euclidean_distance(url_right, url_middle) > 1.5:\n",
    "                        result[\"result\"] = 1\n",
    "                        result[\"reasons\"].append(\"Extra character too far from characters next to it\")\n",
    "                \n",
    "                # prevent the function from running on the rest of the string\n",
    "                break\n",
    "                \n",
    "    elif url_len == l_len:\n",
    "        for i in range(len(l)):\n",
    "            if url[i] != l[i]:\n",
    "                if euclidean_distance(url[i], l[i]) > 1.5:\n",
    "                    result[\"result\"] = 1\n",
    "                    result[\"reasons\"].append(\"'{}' key and '{}' key are too far apart\".format(url[i], l[i]))\n",
    "    \n",
    "    return result\n",
    "\n",
    "for url in typosquat:\n",
    "    # if typo\n",
    "    if is_typo(url) == 0:\n",
    "        print(url)\n",
    "\n",
    "print(is_typo(\"bankofsingapore.org))\n",
    "        \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xn--bankofngapore-up1gx7c.com',\n",
       " 'xn--bankofsigapor-1zb894a.com',\n",
       " 'xn--ankofsingpore-qsb4342i.com',\n",
       " 'xn--bankofsgapore-okb3475i.com',\n",
       " 'xn--barkofsrgapore-r3e.com',\n",
       " 'xn--bnk0fsingp0re-webi.com',\n",
       " 'xn--bankosingapor-gyb5462i.com',\n",
       " 'xn--bankfsimgapore-t88g.com',\n",
       " 'xn--bankofsngapor-o8b053b.com',\n",
       " 'xn--bankfs1ngapre-elbh.com',\n",
       " 'xn--bkofsigapore-ynb0306hfa.com',\n",
       " 'xn--bankfsingape-t5e8723hha.com',\n",
       " 'bankofsingpore.com',\n",
       " 'xn--bnkofsinqapore-vp8g.com',\n",
       " 'xn--bnofsingapore-prb5223i.com',\n",
       " 'xn--bamkofsimgpore-uyf.com',\n",
       " 'xn--bakofsigapore-jkb9973i.com',\n",
       " 'bamk0fsingapore.com',\n",
       " 'xn--bakofsigapre-ql6ft916ofa.com',\n",
       " 'xn--bakofsingpore-xdb1404i.com',\n",
       " 'xn--bamkofsigapore-dpc.com',\n",
       " 'xn--ankofingapore-0rc828a.com',\n",
       " 'xn--bakofsingapre-ckc8744i.com',\n",
       " 'xn--bakofsigapre-ejb79hfa.com',\n",
       " 'xn--bankfingapre-qcdg8351i.com',\n",
       " 'xn--bakfsigapore-d9de0481i.com',\n",
       " 'xn--bankofsnapore-b2b2505i.com',\n",
       " 'xn--bnkofsingpor-pbbi7j.com',\n",
       " 'xn--bnkofsinqapore-bwb.com',\n",
       " 'xn--bankfsingape-6kc41hha.com',\n",
       " 'xn--bakofsigapre-bhbf2f.com',\n",
       " 'xn--bankofslgapore-dpc.com',\n",
       " 'xn--bankofsinapre-6kd13h.com',\n",
       " 'xn--lbnkofsingpore-rhbi.com',\n",
       " 'xn--bankofsigapoe-32fe.com',\n",
       " 'xn--ankofsinapore-wrf5127h.com',\n",
       " 'xn--bnkofsinqpore-3dbi.com',\n",
       " 'xn--bankfsinapore-lmb307a.com',\n",
       " 'xn--bankfingapore-tp1gx9c.com',\n",
       " 'xn--bankofinapore-usc68r.com',\n",
       " 'xn--bkofsigapore-435f2146ofa.com',\n",
       " 'xn--bmkofsimgpore-webi.com',\n",
       " 'xn--bakofsigapre-vdcf2054i.com',\n",
       " 'xn--bankofslngpore-pvb.com',\n",
       " 'xn--bankfsingapre-6kd6513i.com',\n",
       " 'xn--bamkfsingapre-lmbh.com',\n",
       " 'xn--bankofsingpre-yfb560c.com',\n",
       " 'xn--akofsigapore-udcf15g.com',\n",
       " 'xn--bankofsingpoe-0ue17g.com',\n",
       " 'xn--bankofsngapre-z6e46f.com',\n",
       " 'xn--bankofsingore-5wd3202i.com',\n",
       " 'xn--bankofingapoe-1rc057a.com',\n",
       " 'xn--bakofsingaore-rmd5201i.com',\n",
       " 'bankofesingapore.com',\n",
       " 'xn--bakofsigpore-d9df50e.com',\n",
       " 'xn--bnofsingapore-w2d0810i.com',\n",
       " 'xn--bankofsngapre-o8b8094i.com',\n",
       " 'xn--bankofsingpor-yfb72f.com',\n",
       " 'xn--ankofsigapore-9ke65k.com',\n",
       " 'xn--ankofsingapoe-m3f6386h.com',\n",
       " 'xn--bankofimgapore-byc.com',\n",
       " 'xn--bank0fingap0re-byc.com',\n",
       " 'xn--bakofsingaore-5k1gl482p.com',\n",
       " 'xn--bankofsingore-kfb993a.com',\n",
       " 'xn--bakofsigapore-qic4942i.com',\n",
       " 'xn--bankofsingap0e-7u7g.com',\n",
       " 'xn--bakofsigapore-4qcf.com',\n",
       " 'xn--bakofsigapore-je1g7292p.com',\n",
       " 'xn--bnkofsngapore-3db407a.com',\n",
       " 'xn--bankofigapore-up1gp872p.com',\n",
       " 'xn--bnkofsngapore-web955b.com',\n",
       " 'xn--bakofsigapore-ckc4642i.com',\n",
       " 'xn--bankfsingpore-lmb496b.com',\n",
       " 'xn--bankofsngapore-i8fe.com',\n",
       " 'xn--bnkfsingpore-cbbh88z.com',\n",
       " 'xn--banofsingpore-qsb7513i.com',\n",
       " 'xn--bankofsinapor-cde83h.com',\n",
       " 'xn--bakfsingapore-r6e2209h.com',\n",
       " 'xn--bankosigapore-ojc15i.com',\n",
       " 'xn--bankfsigapre-hueg0698h.com',\n",
       " 'xn--bnkofsigpore-gcbh7893i.com',\n",
       " 'xn--bakofsigpore-ncb15kfa.com',\n",
       " 'xn--bakfsigapore-mece575a.com',\n",
       " 'banko.fsingapore.com',\n",
       " 'xn--bkofsigpore-1dbf744bia.com',\n",
       " 'xn--bankofsngapor-1zb767a.com',\n",
       " 'xn--bankofsiqapore-dpc.com',\n",
       " 'xn--ankofsingapor-8gb5682i.com',\n",
       " 'xn--banofsinapore-w3b8672i.com',\n",
       " 'xn--bakofsgapore-xfb3336hfa.com',\n",
       " 'xn--bankofsngapor-2ib02d.com',\n",
       " 'xn--bnkofsingapor-n1b68x.com',\n",
       " 'xn--bakofsingapor-1zb294a.com',\n",
       " 'xn--bnkofsingpre-ejb821aia.com',\n",
       " 'xn--bamkofsmgapore-1mb.com',\n",
       " 'xn--bankfsngapre-hueg97f.com',\n",
       " 'xn--bankfsimgapre-lmbh.com',\n",
       " 'xn--bankofs1ngapoe-7ig.com',\n",
       " 'xn--bankofsingape-tve3241i.com',\n",
       " 'xn--bakofsingaore-ckc71l.com']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(typosquat)[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "<u>Potential Results</u>: <br>\n",
    "0: Typo <br>\n",
    "1: Typosquat\n",
    "\n",
    "<u>Test Cases</u>:\n",
    "1. Legit URL (Expected Result: 0)\n",
    "2. Substitute 1 char with SC (Expected Result: 1)\n",
    "3. Substitute 1 char with hyphen (Expected Result: 1)\n",
    "4. Substitute 1 char with wrong char, within ED boundary (Expected Result: 0)\n",
    "5. Substitute 2 char with wrong char, within ED boundary (Expected Result: 0)\n",
    "6. Substitute 3 char with wrong char, within ED boundary(Expected Result: 1)\n",
    "7. Append 1 char, within ED boundary of last char (Expected Result: 0)\n",
    "8. Append 1 char, exceeding ED boundary (Expected Result: 1)\n",
    "9. Append 2 char, within ED boundary(Expected Result: 1)\n",
    "10. Remove 1 char (Expected Result: 1)\n",
    "    \n",
    "<u>Conclusion/Notes</u>:\n",
    "- need to check entire URL (SLD + TLD) because of potential periods in domain name, which can be removed when extracting SLD\n",
    "- include more checks for suspicious URLs that are shorter than original URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL 0 : 0\n",
      "URL 1 : 1\n",
      "URL 2 : 1\n",
      "URL 3 : 0\n",
      "URL 4 : 0\n",
      "URL 5 : 1\n",
      "URL 6 : 0\n",
      "URL 7 : 1\n",
      "URL 8 : 1\n",
      "URL 9 : 1\n"
     ]
    }
   ],
   "source": [
    "test_cases = [\"bankofsingapore.com\", \n",
    "              \"bankofs!ngapore.com\", \n",
    "              \"bankofsing-pore.com\", \n",
    "              \"bankofsingaporr.com\", \n",
    "              \"bankofsingapoer.com\", \n",
    "              \"bankofsingapier.com\", \n",
    "              \"bankofsingaporee.com\", \n",
    "              \"bankofsingaporep.com\", \n",
    "              \"bankofsingaporeee.com\", \n",
    "              \"bankofsingapor.com\"]\n",
    "\n",
    "for i in range(len(test_cases)):\n",
    "    print(\"URL\", i, \":\", is_typo(test_cases[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
